{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uzIrfB9NZTEJ",
        "outputId": "6a3ed3e7-022b-4f8e-8a1d-66e3db8a43fe"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting torchdata\n",
            "  Downloading torchdata-0.11.0-py3-none-any.whl.metadata (6.3 kB)\n",
            "Requirement already satisfied: urllib3>=1.25 in /usr/local/lib/python3.11/dist-packages (from torchdata) (2.4.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from torchdata) (2.32.3)\n",
            "Requirement already satisfied: torch>=2 in /usr/local/lib/python3.11/dist-packages (from torchdata) (2.6.0+cu124)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch>=2->torchdata) (3.18.0)\n",
            "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.11/dist-packages (from torch>=2->torchdata) (4.13.2)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch>=2->torchdata) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch>=2->torchdata) (3.1.6)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch>=2->torchdata) (2025.3.2)\n",
            "Collecting nvidia-cuda-nvrtc-cu12==12.4.127 (from torch>=2->torchdata)\n",
            "  Downloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-runtime-cu12==12.4.127 (from torch>=2->torchdata)\n",
            "  Downloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-cupti-cu12==12.4.127 (from torch>=2->torchdata)\n",
            "  Downloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cudnn-cu12==9.1.0.70 (from torch>=2->torchdata)\n",
            "  Downloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cublas-cu12==12.4.5.8 (from torch>=2->torchdata)\n",
            "  Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cufft-cu12==11.2.1.3 (from torch>=2->torchdata)\n",
            "  Downloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-curand-cu12==10.3.5.147 (from torch>=2->torchdata)\n",
            "  Downloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cusolver-cu12==11.6.1.9 (from torch>=2->torchdata)\n",
            "  Downloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cusparse-cu12==12.3.1.170 (from torch>=2->torchdata)\n",
            "  Downloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch>=2->torchdata) (0.6.2)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch>=2->torchdata) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=2->torchdata) (12.4.127)\n",
            "Collecting nvidia-nvjitlink-cu12==12.4.127 (from torch>=2->torchdata)\n",
            "  Downloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Requirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch>=2->torchdata) (3.2.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch>=2->torchdata) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch>=2->torchdata) (1.3.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->torchdata) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->torchdata) (3.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->torchdata) (2025.4.26)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch>=2->torchdata) (3.0.2)\n",
            "Downloading torchdata-0.11.0-py3-none-any.whl (61 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.0/62.0 kB\u001b[0m \u001b[31m2.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl (363.4 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m4.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (13.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m56.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (24.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m53.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (883 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m32.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl (664.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m2.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl (211.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m5.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl (56.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m13.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl (127.9 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m7.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl (207.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m5.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (21.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m102.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: nvidia-nvjitlink-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, nvidia-cusparse-cu12, nvidia-cudnn-cu12, nvidia-cusolver-cu12, torchdata\n",
            "  Attempting uninstall: nvidia-nvjitlink-cu12\n",
            "    Found existing installation: nvidia-nvjitlink-cu12 12.5.82\n",
            "    Uninstalling nvidia-nvjitlink-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-nvjitlink-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-curand-cu12\n",
            "    Found existing installation: nvidia-curand-cu12 10.3.6.82\n",
            "    Uninstalling nvidia-curand-cu12-10.3.6.82:\n",
            "      Successfully uninstalled nvidia-curand-cu12-10.3.6.82\n",
            "  Attempting uninstall: nvidia-cufft-cu12\n",
            "    Found existing installation: nvidia-cufft-cu12 11.2.3.61\n",
            "    Uninstalling nvidia-cufft-cu12-11.2.3.61:\n",
            "      Successfully uninstalled nvidia-cufft-cu12-11.2.3.61\n",
            "  Attempting uninstall: nvidia-cuda-runtime-cu12\n",
            "    Found existing installation: nvidia-cuda-runtime-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-runtime-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-runtime-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cuda-nvrtc-cu12\n",
            "    Found existing installation: nvidia-cuda-nvrtc-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-nvrtc-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-nvrtc-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cuda-cupti-cu12\n",
            "    Found existing installation: nvidia-cuda-cupti-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-cupti-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-cupti-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cublas-cu12\n",
            "    Found existing installation: nvidia-cublas-cu12 12.5.3.2\n",
            "    Uninstalling nvidia-cublas-cu12-12.5.3.2:\n",
            "      Successfully uninstalled nvidia-cublas-cu12-12.5.3.2\n",
            "  Attempting uninstall: nvidia-cusparse-cu12\n",
            "    Found existing installation: nvidia-cusparse-cu12 12.5.1.3\n",
            "    Uninstalling nvidia-cusparse-cu12-12.5.1.3:\n",
            "      Successfully uninstalled nvidia-cusparse-cu12-12.5.1.3\n",
            "  Attempting uninstall: nvidia-cudnn-cu12\n",
            "    Found existing installation: nvidia-cudnn-cu12 9.3.0.75\n",
            "    Uninstalling nvidia-cudnn-cu12-9.3.0.75:\n",
            "      Successfully uninstalled nvidia-cudnn-cu12-9.3.0.75\n",
            "  Attempting uninstall: nvidia-cusolver-cu12\n",
            "    Found existing installation: nvidia-cusolver-cu12 11.6.3.83\n",
            "    Uninstalling nvidia-cusolver-cu12-11.6.3.83:\n",
            "      Successfully uninstalled nvidia-cusolver-cu12-11.6.3.83\n",
            "Successfully installed nvidia-cublas-cu12-12.4.5.8 nvidia-cuda-cupti-cu12-12.4.127 nvidia-cuda-nvrtc-cu12-12.4.127 nvidia-cuda-runtime-cu12-12.4.127 nvidia-cudnn-cu12-9.1.0.70 nvidia-cufft-cu12-11.2.1.3 nvidia-curand-cu12-10.3.5.147 nvidia-cusolver-cu12-11.6.1.9 nvidia-cusparse-cu12-12.3.1.170 nvidia-nvjitlink-cu12-12.4.127 torchdata-0.11.0\n"
          ]
        }
      ],
      "source": [
        "!pip install torchdata\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vY4ie5doZiQh",
        "outputId": "e70f414b-629e-44f9-a4a1-0b59d66c7ccc"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting dgl\n",
            "  Downloading dgl-2.1.0-cp311-cp311-manylinux1_x86_64.whl.metadata (553 bytes)\n",
            "Requirement already satisfied: numpy>=1.14.0 in /usr/local/lib/python3.11/dist-packages (from dgl) (2.0.2)\n",
            "Requirement already satisfied: scipy>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from dgl) (1.15.2)\n",
            "Requirement already satisfied: networkx>=2.1 in /usr/local/lib/python3.11/dist-packages (from dgl) (3.4.2)\n",
            "Requirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.11/dist-packages (from dgl) (2.32.3)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from dgl) (4.67.1)\n",
            "Requirement already satisfied: psutil>=5.8.0 in /usr/local/lib/python3.11/dist-packages (from dgl) (5.9.5)\n",
            "Requirement already satisfied: torchdata>=0.5.0 in /usr/local/lib/python3.11/dist-packages (from dgl) (0.11.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests>=2.19.0->dgl) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests>=2.19.0->dgl) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests>=2.19.0->dgl) (2.4.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests>=2.19.0->dgl) (2025.4.26)\n",
            "Requirement already satisfied: torch>=2 in /usr/local/lib/python3.11/dist-packages (from torchdata>=0.5.0->dgl) (2.6.0+cu124)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch>=2->torchdata>=0.5.0->dgl) (3.18.0)\n",
            "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.11/dist-packages (from torch>=2->torchdata>=0.5.0->dgl) (4.13.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch>=2->torchdata>=0.5.0->dgl) (3.1.6)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch>=2->torchdata>=0.5.0->dgl) (2025.3.2)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=2->torchdata>=0.5.0->dgl) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=2->torchdata>=0.5.0->dgl) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=2->torchdata>=0.5.0->dgl) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /usr/local/lib/python3.11/dist-packages (from torch>=2->torchdata>=0.5.0->dgl) (9.1.0.70)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in /usr/local/lib/python3.11/dist-packages (from torch>=2->torchdata>=0.5.0->dgl) (12.4.5.8)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in /usr/local/lib/python3.11/dist-packages (from torch>=2->torchdata>=0.5.0->dgl) (11.2.1.3)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.5.147 in /usr/local/lib/python3.11/dist-packages (from torch>=2->torchdata>=0.5.0->dgl) (10.3.5.147)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in /usr/local/lib/python3.11/dist-packages (from torch>=2->torchdata>=0.5.0->dgl) (11.6.1.9)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in /usr/local/lib/python3.11/dist-packages (from torch>=2->torchdata>=0.5.0->dgl) (12.3.1.170)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch>=2->torchdata>=0.5.0->dgl) (0.6.2)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch>=2->torchdata>=0.5.0->dgl) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=2->torchdata>=0.5.0->dgl) (12.4.127)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=2->torchdata>=0.5.0->dgl) (12.4.127)\n",
            "Requirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch>=2->torchdata>=0.5.0->dgl) (3.2.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch>=2->torchdata>=0.5.0->dgl) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch>=2->torchdata>=0.5.0->dgl) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch>=2->torchdata>=0.5.0->dgl) (3.0.2)\n",
            "Downloading dgl-2.1.0-cp311-cp311-manylinux1_x86_64.whl (8.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m8.6/8.6 MB\u001b[0m \u001b[31m55.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: dgl\n",
            "Successfully installed dgl-2.1.0\n"
          ]
        }
      ],
      "source": [
        "!pip install dgl\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Xh7BeMJ_HBK2",
        "outputId": "540acd00-d667-4116-8f53-d958001028bd"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting dgl==1.0.1\n",
            "  Downloading dgl-1.0.1-cp311-cp311-manylinux1_x86_64.whl.metadata (530 bytes)\n",
            "Requirement already satisfied: numpy>=1.14.0 in /usr/local/lib/python3.11/dist-packages (from dgl==1.0.1) (2.0.2)\n",
            "Requirement already satisfied: scipy>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from dgl==1.0.1) (1.15.2)\n",
            "Requirement already satisfied: networkx>=2.1 in /usr/local/lib/python3.11/dist-packages (from dgl==1.0.1) (3.4.2)\n",
            "Requirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.11/dist-packages (from dgl==1.0.1) (2.32.3)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from dgl==1.0.1) (4.67.1)\n",
            "Requirement already satisfied: psutil>=5.8.0 in /usr/local/lib/python3.11/dist-packages (from dgl==1.0.1) (5.9.5)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests>=2.19.0->dgl==1.0.1) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests>=2.19.0->dgl==1.0.1) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests>=2.19.0->dgl==1.0.1) (2.4.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests>=2.19.0->dgl==1.0.1) (2025.4.26)\n",
            "Downloading dgl-1.0.1-cp311-cp311-manylinux1_x86_64.whl (5.4 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.4/5.4 MB\u001b[0m \u001b[31m25.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: dgl\n",
            "  Attempting uninstall: dgl\n",
            "    Found existing installation: dgl 2.1.0\n",
            "    Uninstalling dgl-2.1.0:\n",
            "      Successfully uninstalled dgl-2.1.0\n",
            "Successfully installed dgl-1.0.1\n"
          ]
        }
      ],
      "source": [
        "!pip install dgl==1.0.1\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aB0oWBOjL3rL",
        "outputId": "dbbbc917-393e-4291-e0f5-2a801ec7461a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "DGL backend not selected or invalid.  Assuming PyTorch for now.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Setting the default backend to \"pytorch\". You can change it in the ~/.dgl/config.json file or export the DGLBACKEND environment variable.  Valid options are: pytorch, mxnet, tensorflow (all lowercase)\n",
            "1.0.1\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import dgl\n",
        "\n",
        "\n",
        "print(dgl.__version__)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "G9l4wTDEPFtR",
        "outputId": "15126aea-2ac8-45b2-d4dc-79503e2dac62"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.11/dist-packages (1.6.1)\n",
            "Requirement already satisfied: numpy>=1.19.5 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (2.0.2)\n",
            "Requirement already satisfied: scipy>=1.6.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (1.15.2)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (1.4.2)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (3.6.0)\n"
          ]
        }
      ],
      "source": [
        "!pip install scikit-learn    # تنصيب مكتبة scikit-learn\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iQTcE_B9vgl4",
        "outputId": "3dea3efd-e323-4d7f-d359-1f8d1df61277"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "\n",
        "# توصيل Google Drive\n",
        "drive.mount('/content/drive', force_remount=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bYkRaOkIS1yI",
        "outputId": "e60d1c5a-dad1-4c74-d783-2a2a0aaf0c06"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n",
            "Books data samples:\n",
            "Sample 1: {'id': '13637412', 'book_name': 'ساق البامبو', 'num_pages': '401', 'publication_year': '2014', 'author_name': 1, 'genres': [1, 2, 3], 'publisher': 1, 'similar_books': None}\n",
            "Sample 2: {'id': '16031620', 'book_name': 'الفيل الأزرق', 'num_pages': '385', 'publication_year': '2012', 'author_name': 2, 'genres': [1, 2, 3], 'publisher': 2, 'similar_books': ['17229930', '9997650', '13562054', '18323154', '11942408', '13645303']}\n",
            "Sample 3: {'id': '9730311', 'book_name': 'ثلاثية غرناطة', 'num_pages': '451', 'publication_year': '1998', 'author_name': 3, 'genres': [1, 2, 3], 'publisher': 2, 'similar_books': ['13562054', '3613975', '9838115', '3776196', '16170625', '7654939']}\n",
            "\n",
            "Users data samples:\n",
            "Sample 1: {'id': '21435637', 'ratings': [{'book_id': '13637412', 'rating': 0}], 'similar_users': [{'user_id': 1719580}, {'user_id': 1733316}, {'user_id': 2511775}, {'user_id': 2749078}, {'user_id': 3213718}, {'user_id': 3360076}, {'user_id': 3598687}, {'user_id': 3854256}, {'user_id': 4139638}, {'user_id': 4327465}, {'user_id': 4434356}, {'user_id': 4503320}, {'user_id': 4800964}, {'user_id': 5073160}, {'user_id': 5153866}, {'user_id': 5157594}, {'user_id': 5195003}, {'user_id': 5287003}, {'user_id': 5506767}, {'user_id': 5632758}, {'user_id': 5716687}, {'user_id': 5722912}, {'user_id': 5779140}, {'user_id': 5791321}, {'user_id': 5827502}, {'user_id': 5896842}, {'user_id': 5906803}, {'user_id': 5974119}, {'user_id': 6145916}, {'user_id': 6260370}, {'user_id': 6672120}, {'user_id': 7279871}, {'user_id': 7296242}, {'user_id': 7297965}, {'user_id': 7483802}, {'user_id': 7506510}, {'user_id': 7576060}, {'user_id': 7608123}, {'user_id': 7611170}, {'user_id': 7937609}, {'user_id': 7971492}, {'user_id': 8096722}, {'user_id': 8110900}, {'user_id': 8112196}, {'user_id': 8120132}, {'user_id': 8370549}, {'user_id': 8464663}, {'user_id': 8532223}, {'user_id': 8664742}, {'user_id': 8836971}, {'user_id': 8865229}, {'user_id': 9840900}, {'user_id': 10498774}, {'user_id': 10568628}, {'user_id': 10619850}, {'user_id': 10767651}, {'user_id': 11367460}, {'user_id': 11751277}, {'user_id': 11811570}, {'user_id': 12120520}, {'user_id': 12136659}, {'user_id': 12448975}, {'user_id': 12570129}, {'user_id': 12609900}, {'user_id': 12724753}, {'user_id': 12849794}, {'user_id': 13095169}, {'user_id': 13147213}, {'user_id': 13216192}, {'user_id': 13401357}, {'user_id': 13678727}, {'user_id': 13774714}, {'user_id': 14245162}, {'user_id': 14396746}, {'user_id': 14887232}, {'user_id': 15176027}, {'user_id': 15315449}, {'user_id': 15381871}, {'user_id': 15413478}, {'user_id': 15431633}, {'user_id': 15454542}, {'user_id': 15559749}, {'user_id': 15652343}, {'user_id': 15738492}, {'user_id': 15770184}, {'user_id': 15984634}, {'user_id': 16052209}, {'user_id': 16179656}, {'user_id': 16466171}, {'user_id': 16614249}, {'user_id': 16684749}, {'user_id': 16815919}, {'user_id': 16964284}, {'user_id': 16979199}, {'user_id': 17035463}, {'user_id': 17046618}, {'user_id': 17158633}, {'user_id': 17171951}, {'user_id': 17557075}, {'user_id': 17559835}, {'user_id': 17688265}, {'user_id': 17805638}, {'user_id': 17811535}, {'user_id': 18175468}, {'user_id': 18200843}, {'user_id': 18238395}, {'user_id': 18435054}, {'user_id': 18842915}, {'user_id': 18940416}, {'user_id': 19416630}, {'user_id': 19541915}, {'user_id': 19905729}, {'user_id': 20044406}, {'user_id': 20284667}, {'user_id': 20359950}, {'user_id': 20426489}, {'user_id': 20449580}, {'user_id': 20747303}, {'user_id': 20748195}, {'user_id': 20751510}, {'user_id': 20912168}, {'user_id': 20954947}, {'user_id': 21034668}, {'user_id': 21039466}, {'user_id': 21096235}, {'user_id': 21198630}, {'user_id': 21211150}, {'user_id': 21281371}, {'user_id': 21365687}, {'user_id': 21468758}, {'user_id': 21725165}, {'user_id': 21759556}, {'user_id': 21825354}, {'user_id': 21972555}, {'user_id': 22119825}, {'user_id': 22177675}, {'user_id': 22245588}, {'user_id': 22368813}, {'user_id': 22375324}, {'user_id': 22703021}, {'user_id': 22781619}, {'user_id': 22808676}, {'user_id': 23083692}, {'user_id': 23399031}, {'user_id': 23585202}, {'user_id': 23778485}, {'user_id': 23827104}, {'user_id': 23922481}, {'user_id': 23951785}, {'user_id': 23954511}, {'user_id': 24398272}, {'user_id': 24454368}, {'user_id': 24814074}, {'user_id': 25242746}, {'user_id': 25513587}, {'user_id': 25527922}, {'user_id': 25666741}, {'user_id': 26544492}, {'user_id': 26736085}, {'user_id': 27645246}, {'user_id': 28091004}, {'user_id': 28108197}, {'user_id': 28340583}, {'user_id': 28343412}, {'user_id': 28412843}, {'user_id': 28560849}, {'user_id': 28750644}, {'user_id': 29153891}, {'user_id': 29429980}, {'user_id': 29593543}, {'user_id': 29834935}, {'user_id': 29850183}, {'user_id': 30046836}, {'user_id': 30834386}, {'user_id': 31246098}, {'user_id': 31274918}, {'user_id': 31300586}, {'user_id': 31346376}, {'user_id': 31382482}, {'user_id': 31621410}, {'user_id': 31989240}, {'user_id': 32344464}, {'user_id': 32404407}, {'user_id': 32444610}, {'user_id': 32473464}, {'user_id': 32630089}, {'user_id': 32714326}, {'user_id': 32720493}, {'user_id': 32802117}, {'user_id': 32896482}, {'user_id': 33005189}, {'user_id': 33018395}, {'user_id': 33150790}, {'user_id': 33183909}, {'user_id': 33257040}, {'user_id': 33299353}, {'user_id': 33402924}, {'user_id': 33588242}, {'user_id': 33607669}, {'user_id': 33629694}, {'user_id': 33723763}, {'user_id': 33858067}, {'user_id': 34185059}, {'user_id': 34301603}, {'user_id': 34485505}, {'user_id': 34673961}, {'user_id': 35396181}, {'user_id': 35889661}, {'user_id': 35921238}, {'user_id': 36015991}, {'user_id': 36080973}, {'user_id': 36089345}, {'user_id': 36106767}, {'user_id': 36182158}, {'user_id': 36502700}, {'user_id': 36529555}, {'user_id': 36585959}, {'user_id': 36591650}, {'user_id': 36607466}, {'user_id': 36726783}, {'user_id': 36805684}, {'user_id': 37011355}, {'user_id': 37080369}, {'user_id': 37202114}, {'user_id': 37208564}, {'user_id': 37406077}, {'user_id': 37419351}, {'user_id': 37457223}, {'user_id': 37647363}, {'user_id': 37650919}, {'user_id': 37891084}, {'user_id': 37903301}, {'user_id': 38084106}, {'user_id': 38150983}, {'user_id': 38843959}, {'user_id': 38870659}, {'user_id': 39291876}, {'user_id': 39446880}, {'user_id': 39739396}, {'user_id': 39761819}, {'user_id': 40056240}, {'user_id': 40295503}, {'user_id': 40488636}, {'user_id': 40489990}, {'user_id': 40687714}, {'user_id': 40689792}, {'user_id': 40997293}, {'user_id': 41029766}, {'user_id': 41157080}, {'user_id': 41236257}, {'user_id': 41675285}, {'user_id': 41876184}, {'user_id': 42169097}, {'user_id': 42302956}, {'user_id': 42662305}, {'user_id': 43301353}, {'user_id': 43351626}, {'user_id': 43438091}, {'user_id': 43773944}, {'user_id': 43876495}, {'user_id': 44147167}, {'user_id': 44697907}, {'user_id': 45242846}, {'user_id': 45307724}, {'user_id': 45309356}, {'user_id': 45429385}, {'user_id': 46015702}, {'user_id': 46058422}, {'user_id': 46106392}, {'user_id': 46219267}, {'user_id': 46598987}, {'user_id': 46717480}, {'user_id': 47838482}, {'user_id': 48211249}, {'user_id': 48465569}, {'user_id': 49834580}, {'user_id': 50139854}, {'user_id': 50285115}, {'user_id': 50348342}, {'user_id': 50403159}, {'user_id': 50563670}, {'user_id': 50826803}, {'user_id': 50960049}, {'user_id': 50989012}, {'user_id': 51293754}, {'user_id': 51598359}, {'user_id': 52199940}, {'user_id': 52575877}, {'user_id': 52898040}, {'user_id': 53278067}, {'user_id': 54025730}, {'user_id': 54765823}, {'user_id': 55538507}, {'user_id': 55957075}, {'user_id': 56051751}, {'user_id': 56141422}, {'user_id': 56445490}, {'user_id': 56517018}, {'user_id': 56708853}]}\n",
            "Sample 2: {'id': '20015365', 'ratings': [{'book_id': '13637412', 'rating': 0}], 'similar_users': [{'user_id': 14030463}, {'user_id': 23331413}, {'user_id': 26940796}]}\n",
            "Sample 3: {'id': '56517018', 'ratings': [{'book_id': '13637412', 'rating': 0}], 'similar_users': [{'user_id': 1719580}, {'user_id': 1733316}, {'user_id': 2024457}, {'user_id': 2511775}, {'user_id': 2749078}, {'user_id': 3360076}, {'user_id': 3598687}, {'user_id': 3854256}, {'user_id': 4503320}, {'user_id': 5073160}, {'user_id': 5153866}, {'user_id': 5157594}, {'user_id': 5195003}, {'user_id': 5287003}, {'user_id': 5506767}, {'user_id': 5632758}, {'user_id': 5716687}, {'user_id': 5779140}, {'user_id': 5791321}, {'user_id': 5827502}, {'user_id': 5896842}, {'user_id': 6672120}, {'user_id': 7279871}, {'user_id': 7297965}, {'user_id': 7506510}, {'user_id': 7576060}, {'user_id': 7611170}, {'user_id': 7937609}, {'user_id': 7971492}, {'user_id': 8096722}, {'user_id': 8112196}, {'user_id': 8120132}, {'user_id': 8370549}, {'user_id': 8463552}, {'user_id': 8532223}, {'user_id': 8865229}, {'user_id': 10498774}, {'user_id': 10568628}, {'user_id': 10619850}, {'user_id': 10767651}, {'user_id': 11367460}, {'user_id': 11552935}, {'user_id': 11751277}, {'user_id': 12120520}, {'user_id': 12136659}, {'user_id': 12570129}, {'user_id': 12609900}, {'user_id': 12724753}, {'user_id': 13095169}, {'user_id': 13147213}, {'user_id': 13216192}, {'user_id': 13401357}, {'user_id': 13678727}, {'user_id': 14245162}, {'user_id': 14887232}, {'user_id': 15049942}, {'user_id': 15176027}, {'user_id': 15381871}, {'user_id': 15431633}, {'user_id': 15559749}, {'user_id': 15652343}, {'user_id': 15770184}, {'user_id': 15984634}, {'user_id': 16052209}, {'user_id': 16179656}, {'user_id': 16466171}, {'user_id': 16614249}, {'user_id': 16684749}, {'user_id': 16815919}, {'user_id': 16964284}, {'user_id': 16979199}, {'user_id': 17035463}, {'user_id': 17046618}, {'user_id': 17075647}, {'user_id': 17158633}, {'user_id': 17555061}, {'user_id': 17557075}, {'user_id': 17559835}, {'user_id': 17688265}, {'user_id': 18238395}, {'user_id': 18940416}, {'user_id': 19416630}, {'user_id': 19541915}, {'user_id': 19736603}, {'user_id': 20044406}, {'user_id': 20284667}, {'user_id': 20324477}, {'user_id': 20359950}, {'user_id': 20449580}, {'user_id': 20747303}, {'user_id': 20751510}, {'user_id': 20912168}, {'user_id': 20954947}, {'user_id': 21034668}, {'user_id': 21039466}, {'user_id': 21198630}, {'user_id': 21211150}, {'user_id': 21365687}, {'user_id': 21435637}, {'user_id': 21468758}, {'user_id': 21705768}, {'user_id': 21725165}, {'user_id': 21759556}, {'user_id': 21825354}, {'user_id': 22119825}, {'user_id': 22177675}, {'user_id': 22368813}, {'user_id': 22703021}, {'user_id': 22781619}, {'user_id': 22808676}, {'user_id': 23083692}, {'user_id': 23399031}, {'user_id': 23585202}, {'user_id': 23778485}, {'user_id': 23951785}, {'user_id': 23954511}, {'user_id': 24398272}, {'user_id': 25242746}, {'user_id': 25527922}, {'user_id': 25666741}, {'user_id': 26544492}, {'user_id': 26736085}, {'user_id': 27645246}, {'user_id': 28091004}, {'user_id': 28108197}, {'user_id': 28340583}, {'user_id': 28343412}, {'user_id': 28412843}, {'user_id': 28560849}, {'user_id': 28750644}, {'user_id': 29429980}, {'user_id': 29593543}, {'user_id': 29834935}, {'user_id': 29850183}, {'user_id': 30834386}, {'user_id': 31246098}, {'user_id': 31274918}, {'user_id': 31300586}, {'user_id': 31346376}, {'user_id': 31382482}, {'user_id': 31621410}, {'user_id': 31719381}, {'user_id': 32344464}, {'user_id': 32404407}, {'user_id': 32473464}, {'user_id': 32630089}, {'user_id': 32714326}, {'user_id': 32720493}, {'user_id': 32802117}, {'user_id': 32896482}, {'user_id': 33005189}, {'user_id': 33018395}, {'user_id': 33183909}, {'user_id': 33257040}, {'user_id': 33299353}, {'user_id': 33588242}, {'user_id': 33607669}, {'user_id': 33629694}, {'user_id': 33723763}, {'user_id': 33858067}, {'user_id': 34109830}, {'user_id': 34185059}, {'user_id': 34673961}, {'user_id': 35396181}, {'user_id': 35889661}, {'user_id': 36015991}, {'user_id': 36080973}, {'user_id': 36182158}, {'user_id': 36344385}, {'user_id': 36502700}, {'user_id': 36529555}, {'user_id': 36585959}, {'user_id': 36591650}, {'user_id': 36607466}, {'user_id': 36726783}, {'user_id': 36805684}, {'user_id': 37011355}, {'user_id': 37080369}, {'user_id': 37202114}, {'user_id': 37406077}, {'user_id': 37419351}, {'user_id': 37457223}, {'user_id': 37647363}, {'user_id': 37650919}, {'user_id': 37736877}, {'user_id': 37891084}, {'user_id': 38843959}, {'user_id': 38870659}, {'user_id': 39291876}, {'user_id': 39446880}, {'user_id': 39739396}, {'user_id': 39761819}, {'user_id': 40056240}, {'user_id': 40220435}, {'user_id': 40488636}, {'user_id': 40489990}, {'user_id': 40687714}, {'user_id': 40689792}, {'user_id': 40843002}, {'user_id': 40997293}, {'user_id': 41029766}, {'user_id': 41157080}, {'user_id': 41207394}, {'user_id': 41675285}, {'user_id': 41876184}, {'user_id': 42302956}, {'user_id': 42504767}, {'user_id': 42662305}, {'user_id': 43301353}, {'user_id': 43351626}, {'user_id': 43773944}, {'user_id': 44147167}, {'user_id': 45242846}, {'user_id': 45307724}, {'user_id': 45309356}, {'user_id': 45429385}, {'user_id': 46015702}, {'user_id': 46058422}, {'user_id': 46219267}, {'user_id': 47838482}, {'user_id': 48211249}, {'user_id': 50139854}, {'user_id': 50285115}, {'user_id': 50403159}, {'user_id': 50826803}, {'user_id': 50960049}, {'user_id': 51188274}, {'user_id': 51598359}, {'user_id': 52199940}, {'user_id': 52575877}, {'user_id': 52898040}, {'user_id': 54025730}, {'user_id': 54765823}, {'user_id': 56445490}, {'user_id': 56708853}]}\n",
            "\n",
            "Authors data samples:\n",
            "Sample 1: {'id': 1, 'name': 'سعود السنعوسي'}\n",
            "Sample 2: {'id': 2, 'name': 'أحمد مراد'}\n",
            "Sample 3: {'id': 3, 'name': 'رضوى عاشور'}\n",
            "\n",
            "Genres data samples:\n",
            "Sample 1: {'id': 1, 'name': 'روايات وقصص'}\n",
            "Sample 2: {'id': 2, 'name': 'خيالي'}\n",
            "Sample 3: {'id': 3, 'name': 'روايات'}\n",
            "\n",
            "Publishers data samples:\n",
            "Sample 1: {'id': 1, 'name': 'الدار العربية للعلوم ناشرون'}\n",
            "Sample 2: {'id': 2, 'name': 'دار الشروق'}\n",
            "Sample 3: {'id': 3, 'name': 'دار الكرمة'}\n"
          ]
        }
      ],
      "source": [
        "# -*- coding: utf-8 -*-\n",
        "\"\"\"\n",
        "Annotated Data Loading and Validation Script\n",
        "Purpose\n",
        "-------\n",
        "Load several JSON datasets (books, users, authors, genres, publishers) from\n",
        "Google Drive, verify that every record contains the expected keys, insert safe\n",
        "defaults where data are missing, and print a few samples .\n",
        "\"\"\"\n",
        "import json\n",
        "from google.colab import drive\n",
        "\n",
        "# توصيل Google Drive\n",
        "drive.mount('/content/drive', force_remount=True)\n",
        "\n",
        "# Reading and Validating JSON Files\n",
        "def read_and_validate_json(file_path, expected_keys=None):\n",
        "    valid_data = []\n",
        "    try:\n",
        "        with open(file_path, 'r', encoding='utf-8') as f:\n",
        "            data = json.load(f)\n",
        "            for item in data:\n",
        "                valid_item = {}\n",
        "                if expected_keys:\n",
        "                    for key in expected_keys:\n",
        "                        if key in item:\n",
        "                            valid_item[key] = item[key]\n",
        "                        else:\n",
        "                            print(f\"Warning: Missing key '{key}' in item {item}. Adding default value.\")\n",
        "                            valid_item[key] = get_default_value(key)\n",
        "                valid_data.append(valid_item)\n",
        "            return valid_data\n",
        "    except (json.JSONDecodeError, Exception) as e:\n",
        "        print(f\"Error reading {file_path}: {e}\")\n",
        "        return []\n",
        "\n",
        "def get_default_value(key):\n",
        "    # تخصيص القيم الافتراضية بناءً على نوع البيانات\n",
        "    default_values = {\n",
        "        'id': '0',\n",
        "        'num_pages': 0,\n",
        "        'book_name': 'unknown',\n",
        "        'publication_year': 0,\n",
        "        'author_name': '',\n",
        "        'genres': [],\n",
        "        'publisher': '',\n",
        "        'similar_books': [],\n",
        "        'ratings': [],\n",
        "        'similar_users': []\n",
        "    }\n",
        "    return default_values.get(key, None)\n",
        "\n",
        "def load_and_validate_data(file_paths):\n",
        "    books_data = read_and_validate_json(file_paths['books'], expected_keys=['id', 'book_name', 'num_pages', 'publication_year', 'author_name', 'genres', 'publisher', 'similar_books'])\n",
        "    users_data = read_and_validate_json(file_paths['users'], expected_keys=['id', 'ratings', 'similar_users'])\n",
        "    authors_data = read_and_validate_json(file_paths['authors'], expected_keys=['id', 'name'])\n",
        "    genres_data = read_and_validate_json(file_paths['genres'], expected_keys=['id', 'name'])\n",
        "    publishers_data = read_and_validate_json(file_paths['publishers'], expected_keys=['id', 'name'])\n",
        "\n",
        "    return books_data, users_data, authors_data, genres_data, publishers_data\n",
        "\n",
        "# طباعة بعض العينات للتحقق\n",
        "def print_sample_data(data, sample_size=3):\n",
        "    for i, item in enumerate(data[:sample_size]):\n",
        "        print(f\"Sample {i+1}: {item}\")\n",
        "\n",
        "\n",
        "file_paths = {\n",
        "    \"books\": '/content/drive/MyDrive/العملي/filtered_books.json',\n",
        "    \"users\": '/content/drive/MyDrive/العملي/modified_users_graph.json',\n",
        "    \"authors\": '/content/drive/MyDrive/العملي/authors.json',\n",
        "    \"genres\": '/content/drive/MyDrive/العملي/genres.json',\n",
        "    \"publishers\": '/content/drive/MyDrive/العملي/publishers.json'\n",
        "}\n",
        "\n",
        "books_data, users_data, authors_data, genres_data, publishers_data = load_and_validate_data(file_paths)\n",
        "\n",
        "# # Print samples to verify data after verification and processing.\n",
        "print(\"Books data samples:\")\n",
        "print_sample_data(books_data)\n",
        "\n",
        "print(\"\\nUsers data samples:\")\n",
        "print_sample_data(users_data)\n",
        "\n",
        "print(\"\\nAuthors data samples:\")\n",
        "print_sample_data(authors_data)\n",
        "\n",
        "print(\"\\nGenres data samples:\")\n",
        "print_sample_data(genres_data)\n",
        "\n",
        "print(\"\\nPublishers data samples:\")\n",
        "print_sample_data(publishers_data)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qajVXGW43ony",
        "outputId": "cfd4b98c-a2d3-4dcb-afe0-2ef7d0f36d96"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Graph with features and 'nid' saved successfully.\n",
            "Graph Summary:\n",
            "====================\n",
            "Number of nodes:\n",
            "  - author: 2021\n",
            "  - book: 4957\n",
            "  - genre: 165\n",
            "  - publisher: 222\n",
            "  - user: 44490\n",
            "\n",
            "Number of edges:\n",
            "  - ('author', 'wrote', 'book'): 4945\n",
            "  - ('book', 'categorized_as', 'genre'): 12717\n",
            "  - ('book', 'published_by', 'publisher'): 4916\n",
            "  - ('book', 'rated_by', 'user'): 156500\n",
            "  - ('book', 'written_by', 'author'): 4945\n",
            "  - ('genre', 'categorized_in', 'book'): 12717\n",
            "  - ('publisher', 'publishes', 'book'): 4916\n",
            "  - ('user', 'rated', 'book'): 156500\n",
            "  - ('user', 'similar_to', 'user'): 742039\n",
            "\n",
            "Node features:\n",
            "Node type: author\n",
            "  Features: ['original_id', 'nid']\n",
            "    - original_id: shape torch.Size([2021]), dtype torch.int32\n",
            "    - nid: shape torch.Size([2021]), dtype torch.int32\n",
            "Node type: book\n",
            "  Features: ['features', 'original_id', 'nid']\n",
            "    - features: shape torch.Size([4957, 3]), dtype torch.float32\n",
            "    - original_id: shape torch.Size([4957]), dtype torch.int32\n",
            "    - nid: shape torch.Size([4957]), dtype torch.int32\n",
            "Node type: genre\n",
            "  Features: ['original_id', 'nid']\n",
            "    - original_id: shape torch.Size([165]), dtype torch.int32\n",
            "    - nid: shape torch.Size([165]), dtype torch.int32\n",
            "Node type: publisher\n",
            "  Features: ['original_id', 'nid']\n",
            "    - original_id: shape torch.Size([222]), dtype torch.int32\n",
            "    - nid: shape torch.Size([222]), dtype torch.int32\n",
            "Node type: user\n",
            "  Features: ['original_id', 'nid']\n",
            "    - original_id: shape torch.Size([44490]), dtype torch.int32\n",
            "    - nid: shape torch.Size([44490]), dtype torch.int32\n",
            "\n",
            "Edge features:\n",
            "Edge type: ('author', 'wrote', 'book')\n",
            "  Features: []\n",
            "Edge type: ('book', 'categorized_as', 'genre')\n",
            "  Features: []\n",
            "Edge type: ('book', 'published_by', 'publisher')\n",
            "  Features: []\n",
            "Edge type: ('book', 'rated_by', 'user')\n",
            "  Features: ['rating']\n",
            "    - rating: shape torch.Size([156500]), dtype torch.float32\n",
            "Edge type: ('book', 'written_by', 'author')\n",
            "  Features: []\n",
            "Edge type: ('genre', 'categorized_in', 'book')\n",
            "  Features: []\n",
            "Edge type: ('publisher', 'publishes', 'book')\n",
            "  Features: []\n",
            "Edge type: ('user', 'rated', 'book')\n",
            "  Features: ['rating']\n",
            "    - rating: shape torch.Size([156500]), dtype torch.float32\n",
            "Edge type: ('user', 'similar_to', 'user')\n",
            "  Features: []\n",
            "\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# -*- coding: utf-8 -*-\n",
        "\"\"\"\n",
        "HeteroGraph Builder\n",
        "=========================================\n",
        "Summary\n",
        "-------\n",
        "* Loads five JSON datasets (books, users, authors, genres, publishers) from\n",
        "  Google Drive.\n",
        "* Validates that every record contains all required keys and inserts sensible\n",
        "  defaults whenever data are missing.\n",
        "* Builds a **DGL HeteroGraph** that links users to books, authors, genres, and\n",
        "  publishers, and attaches user‑to‑user similarity edges.\n",
        "* Stores rating values on the corresponding edges and a small numeric feature\n",
        "  vector on each book node (page count, publication year, book id).\n",
        "* Saves the finished graph and prints a concise inspection report so you can\n",
        "  verify its structure.\n",
        "\"\"\"\n",
        "import json\n",
        "import dgl\n",
        "import torch\n",
        "import os\n",
        "\n",
        "##Read JSON files\n",
        "file_paths = {\n",
        "    \"books\": '/content/drive/MyDrive/العملي/filtered_books.json',\n",
        "    \"users\": '/content/drive/MyDrive/العملي/modified_users_graph.json',\n",
        "    \"authors\": '/content/drive/MyDrive/العملي/authors.json',\n",
        "    \"genres\": '/content/drive/MyDrive/العملي/genres.json',\n",
        "    \"publishers\": '/content/drive/MyDrive/العملي/publishers.json'\n",
        "}\n",
        "\n",
        "def read_and_validate_json(file_path, expected_keys=None):\n",
        "    valid_data = []\n",
        "    try:\n",
        "        with open(file_path, 'r', encoding='utf-8') as f:\n",
        "            data = json.load(f)\n",
        "            for item in data:\n",
        "                valid_item = {}\n",
        "                if expected_keys:\n",
        "                    for key in expected_keys:\n",
        "                        if key in item:\n",
        "                            valid_item[key] = item[key]\n",
        "                        else:\n",
        "                            print(f\"Warning: Missing key '{key}' in item {item}. Adding default value.\")\n",
        "                            valid_item[key] = get_default_value(key)\n",
        "                valid_data.append(valid_item)\n",
        "            return valid_data\n",
        "    except (json.JSONDecodeError, Exception) as e:\n",
        "        print(f\"Error reading {file_path}: {e}\")\n",
        "        return []\n",
        "\n",
        "def get_default_value(key):\n",
        "    default_values = {\n",
        "        'id': '0',\n",
        "        'num_pages': 0,\n",
        "        'publication_year': 0,\n",
        "        'author_name': '',\n",
        "        'genres': [],\n",
        "        'publisher': '',\n",
        "        'similar_books': [],\n",
        "        'ratings': [],\n",
        "        'similar_users': [],\n",
        "        'name': 'unknown'\n",
        "    }\n",
        "    return default_values.get(key, None)\n",
        "\n",
        "def load_and_validate_data(file_paths):\n",
        "    books_data = read_and_validate_json(file_paths['books'], expected_keys=['id', 'num_pages', 'publication_year', 'author_name', 'genres', 'publisher', 'similar_books'])\n",
        "    users_data = read_and_validate_json(file_paths['users'], expected_keys=['id', 'ratings', 'similar_users'])\n",
        "    authors_data = read_and_validate_json(file_paths['authors'], expected_keys=['id', 'name'])\n",
        "    genres_data = read_and_validate_json(file_paths['genres'], expected_keys=['id', 'name'])\n",
        "    publishers_data = read_and_validate_json(file_paths['publishers'], expected_keys=['id', 'name'])\n",
        "\n",
        "    return books_data, users_data, authors_data, genres_data, publishers_data\n",
        "\n",
        "\n",
        "def build_heterogeneous_graph(books_data, users_data, authors_data, genres_data, publishers_data):\n",
        "    #Create ID maps for each node type and convert them to String type to ensure compatibility.\n",
        "    user_ids = {str(user['id']): i for i, user in enumerate(users_data)}\n",
        "    book_ids = {str(book['id']): i for i, book in enumerate(books_data)}\n",
        "    author_ids = {str(author['id']): i for i, author in enumerate(authors_data)}\n",
        "    genre_ids = {str(genre['id']): i for i, genre in enumerate(genres_data)}\n",
        "    publisher_ids = {str(publisher['id']): i for i, publisher in enumerate(publishers_data)}\n",
        "\n",
        "    data_dict = {\n",
        "        ('user', 'rated', 'book'): [],\n",
        "        ('book', 'rated_by', 'user'): [],\n",
        "        ('book', 'written_by', 'author'): [],\n",
        "        ('author', 'wrote', 'book'): [],\n",
        "        ('book', 'categorized_as', 'genre'): [],\n",
        "        ('genre', 'categorized_in', 'book'): [],\n",
        "        ('book', 'published_by', 'publisher'): [],\n",
        "        ('publisher', 'publishes', 'book'): [],\n",
        "        ('user', 'similar_to', 'user'): [],\n",
        "    }\n",
        "\n",
        "    rating_values = []\n",
        "    similarity_values = []  # List of similarity values ​​for 'similar to' edges\n",
        "\n",
        "    #Create edges between users and books\n",
        "    added_pairs = set()  # To store added pairs and check for duplication\n",
        "    for user in users_data:\n",
        "        user_idx = user_ids.get(str(user['id']))\n",
        "        if user_idx is None:\n",
        "            print(f\"تنبيه: لم يتم العثور على معرف المستخدم {user['id']} في user_ids.\")\n",
        "            continue\n",
        "        if 'ratings' in user:\n",
        "            for rating in user['ratings']:\n",
        "                book_idx = book_ids.get(str(rating['book_id']))\n",
        "                if book_idx is not None:\n",
        "                    data_dict[('user', 'rated', 'book')].append((user_idx, book_idx))\n",
        "                    data_dict[('book', 'rated_by', 'user')].append((book_idx, user_idx))\n",
        "                    rating_values.append(rating['rating'])\n",
        "\n",
        "        if 'similar_users' in user:\n",
        "            for similar_user in user['similar_users']:\n",
        "                similar_user_id = str(similar_user['user_id'])\n",
        "                similar_user_idx = user_ids.get(similar_user_id)\n",
        "               # similarity_score = similar_user['similarity']\n",
        "\n",
        "                if similar_user_idx is not None:\n",
        "                    # ترتيب الزوج لضمان إضافة العلاقة مرة واحدة\n",
        "                    pair = tuple(sorted((user_idx, similar_user_idx)))\n",
        "                    if pair not in added_pairs:\n",
        "                        added_pairs.add(pair)  # أضف الزوج لمجموعة الأزواج المضافة\n",
        "                        data_dict[('user', 'similar_to', 'user')].append(pair)\n",
        "                       # similarity_values.append(similarity_score)\n",
        "                else:\n",
        "                    print(f\"تنبيه: لم يتم العثور على معرف المستخدم المشابه {similar_user_id} في user_ids.\")\n",
        "\n",
        "    # Create edges between books and other entities\n",
        "    for book in books_data:\n",
        "        book_idx = book_ids.get(str(book['id']))\n",
        "        if book_idx is None:\n",
        "            print(f\"تنبيه: لم يتم العثور على معرف الكتاب {book['id']} في book_ids.\")\n",
        "            continue\n",
        "        if 'author_name' in book and str(book['author_name']) in author_ids:\n",
        "            author_idx = author_ids[str(book['author_name'])]\n",
        "            data_dict[('book', 'written_by', 'author')].append((book_idx, author_idx))\n",
        "            data_dict[('author', 'wrote', 'book')].append((author_idx, book_idx))\n",
        "        if 'genres' in book and isinstance(book['genres'], list):\n",
        "            for genre_id in book['genres']:\n",
        "                genre_idx = genre_ids.get(str(genre_id))\n",
        "                if genre_idx is not None:\n",
        "                    data_dict[('book', 'categorized_as', 'genre')].append((book_idx, genre_idx))\n",
        "                    data_dict[('genre', 'categorized_in', 'book')].append((genre_idx, book_idx))\n",
        "        if 'publisher' in book and str(book['publisher']) in publisher_ids:\n",
        "            publisher_idx = publisher_ids[str(book['publisher'])]\n",
        "            data_dict[('book', 'published_by', 'publisher')].append((book_idx, publisher_idx))\n",
        "            data_dict[('publisher', 'publishes', 'book')].append((publisher_idx, book_idx))\n",
        "\n",
        "    G = dgl.heterograph(data_dict)\n",
        "\n",
        "    # Add rating values ​​to edges\n",
        "    G.edges[('user', 'rated', 'book')].data['rating'] = torch.tensor(rating_values, dtype=torch.float32)\n",
        "    G.edges[('book', 'rated_by', 'user')].data['rating'] = torch.tensor(rating_values, dtype=torch.float32)\n",
        "\n",
        "    # Add features to book node\n",
        "    G.nodes['book'].data['features'] = torch.tensor(\n",
        "        [[\n",
        "            float(book['num_pages']) if book['num_pages'] and str(book['num_pages']).isdigit() else 0.0,\n",
        "            float(book['publication_year']) if book['publication_year'] and str(book['publication_year']).isdigit() else 0.0,\n",
        "            float(book['id']) if book['id'] and str(book['id']).isdigit() else 0.0\n",
        "        ] for book in books_data],\n",
        "        dtype=torch.float32\n",
        "    )\n",
        "\n",
        "    # Add original id for each node type.\n",
        "    G.nodes['book'].data['original_id'] = torch.tensor([int(book['id']) for book in books_data], dtype=torch.int32)\n",
        "    G.nodes['user'].data['original_id'] = torch.tensor([int(user['id']) for user in users_data], dtype=torch.int32)\n",
        "    G.nodes['author'].data['original_id'] = torch.tensor([int(author['id']) for author in authors_data], dtype=torch.int32)\n",
        "    G.nodes['genre'].data['original_id'] = torch.tensor([int(genre['id']) for genre in genres_data], dtype=torch.int32)\n",
        "    G.nodes['publisher'].data['original_id'] = torch.tensor([int(publisher['id']) for publisher in publishers_data], dtype=torch.int32)\n",
        "\n",
        "    for ntype in G.ntypes:\n",
        "        num_nodes = G.num_nodes(ntype)\n",
        "        G.nodes[ntype].data['nid'] = torch.arange(num_nodes, dtype=torch.int32)\n",
        "\n",
        "    return G\n",
        "\n",
        "\n",
        "def inspect_graph(graph):\n",
        "    print(\"Graph Summary:\")\n",
        "    print(\"=\" * 20)\n",
        "    print(\"Number of nodes:\")\n",
        "    for ntype in graph.ntypes:\n",
        "        print(f\"  - {ntype}: {graph.num_nodes(ntype)}\")\n",
        "    print(\"\\nNumber of edges:\")\n",
        "    for etype in graph.canonical_etypes:\n",
        "        print(f\"  - {etype}: {graph.num_edges(etype)}\")\n",
        "    print(\"\\nNode features:\")\n",
        "    for ntype in graph.ntypes:\n",
        "        print(f\"Node type: {ntype}\")\n",
        "        node_data = graph.nodes[ntype].data\n",
        "        print(f\"  Features: {list(node_data.keys())}\")\n",
        "        for key in node_data.keys():\n",
        "            print(f\"    - {key}: shape {node_data[key].shape}, dtype {node_data[key].dtype}\")\n",
        "    print(\"\\nEdge features:\")\n",
        "    for etype in graph.canonical_etypes:\n",
        "        print(f\"Edge type: {etype}\")\n",
        "        edge_data = graph.edges[etype].data\n",
        "        print(f\"  Features: {list(edge_data.keys())}\")\n",
        "        for key in edge_data.keys():\n",
        "            print(f\"    - {key}: shape {edge_data[key].shape}, dtype {edge_data[key].dtype}\")\n",
        "    print(\"\\n\")\n",
        "\n",
        "def main():\n",
        "    # =Load data\n",
        "    books_data, users_data, authors_data, genres_data, publishers_data = load_and_validate_data(file_paths)\n",
        "\n",
        "    # Build a graph with features and add 'nid'\n",
        "    graph = build_heterogeneous_graph(books_data, users_data, authors_data, genres_data, publishers_data)\n",
        "\n",
        "    # Save the graph\n",
        "    output_graph_path = '/content/drive/MyDrive/graph/heterogeneous_graph_bert.bin'\n",
        "    dgl.save_graphs(output_graph_path, [graph])\n",
        "    print(\"Graph with features and 'nid' saved successfully.\")\n",
        "\n",
        "    # Print graph specifications\n",
        "    inspect_graph(graph)\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3cz_dvBzvTwz",
        "outputId": "c0fa72a4-c0b9-417a-cb15-9641e46cdd62"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "تم إنشاء تضمينات عشوائية قابلة للتدريب للعقد من النوع 'author'.\n",
            "تم توسيع ميزات العقد من النوع 'book' إلى الأبعاد 64.\n",
            "تم إنشاء تضمينات عشوائية قابلة للتدريب للعقد من النوع 'genre'.\n",
            "تم إنشاء تضمينات عشوائية قابلة للتدريب للعقد من النوع 'publisher'.\n",
            "تم إنشاء تضمينات عشوائية قابلة للتدريب للعقد من النوع 'user'.\n",
            "\n",
            "Graph Summary: Training Graph\n",
            "=====================\n",
            "Number of nodes:\n",
            "  - author: 2021\n",
            "  - book: 4957\n",
            "  - genre: 165\n",
            "  - publisher: 222\n",
            "  - user: 44490\n",
            "\n",
            "Number of edges:\n",
            "  - ('author', 'wrote', 'book'): 4945\n",
            "  - ('book', 'categorized_as', 'genre'): 12717\n",
            "  - ('book', 'published_by', 'publisher'): 4916\n",
            "  - ('book', 'rated_by', 'user'): 125361\n",
            "  - ('book', 'written_by', 'author'): 4945\n",
            "  - ('genre', 'categorized_in', 'book'): 12717\n",
            "  - ('publisher', 'publishes', 'book'): 4916\n",
            "  - ('user', 'rated', 'book'): 125433\n",
            "  - ('user', 'similar_to', 'user'): 742039\n",
            "\n",
            "Node features:\n",
            "Node type: author\n",
            "  Features: ['nid', 'original_id', 'feat']\n",
            "    - nid: shape torch.Size([2021])\n",
            "    - original_id: shape torch.Size([2021])\n",
            "    - feat: shape torch.Size([2021, 64])\n",
            "Node type: book\n",
            "  Features: ['original_id', 'nid', 'features', 'feat']\n",
            "    - original_id: shape torch.Size([4957])\n",
            "    - nid: shape torch.Size([4957])\n",
            "    - features: shape torch.Size([4957, 3])\n",
            "    - feat: shape torch.Size([4957, 64])\n",
            "Node type: genre\n",
            "  Features: ['nid', 'original_id', 'feat']\n",
            "    - nid: shape torch.Size([165])\n",
            "    - original_id: shape torch.Size([165])\n",
            "    - feat: shape torch.Size([165, 64])\n",
            "Node type: publisher\n",
            "  Features: ['nid', 'original_id', 'feat']\n",
            "    - nid: shape torch.Size([222])\n",
            "    - original_id: shape torch.Size([222])\n",
            "    - feat: shape torch.Size([222, 64])\n",
            "Node type: user\n",
            "  Features: ['nid', 'original_id', 'feat']\n",
            "    - nid: shape torch.Size([44490])\n",
            "    - original_id: shape torch.Size([44490])\n",
            "    - feat: shape torch.Size([44490, 64])\n",
            "\n",
            "Edge features:\n",
            "Edge type: ('author', 'wrote', 'book')\n",
            "Edge type: ('book', 'categorized_as', 'genre')\n",
            "Edge type: ('book', 'published_by', 'publisher')\n",
            "Edge type: ('book', 'rated_by', 'user')\n",
            "    - rating: shape torch.Size([125361])\n",
            "Edge type: ('book', 'written_by', 'author')\n",
            "Edge type: ('genre', 'categorized_in', 'book')\n",
            "Edge type: ('publisher', 'publishes', 'book')\n",
            "Edge type: ('user', 'rated', 'book')\n",
            "    - rating: shape torch.Size([125433])\n",
            "Edge type: ('user', 'similar_to', 'user')\n",
            "\n",
            "\n",
            "\n",
            "Graph Summary: Validation Graph\n",
            "=====================\n",
            "Number of nodes:\n",
            "  - author: 2021\n",
            "  - book: 4957\n",
            "  - genre: 165\n",
            "  - publisher: 222\n",
            "  - user: 44490\n",
            "\n",
            "Number of edges:\n",
            "  - ('author', 'wrote', 'book'): 4945\n",
            "  - ('book', 'categorized_as', 'genre'): 12717\n",
            "  - ('book', 'published_by', 'publisher'): 4916\n",
            "  - ('book', 'rated_by', 'user'): 16004\n",
            "  - ('book', 'written_by', 'author'): 4945\n",
            "  - ('genre', 'categorized_in', 'book'): 12717\n",
            "  - ('publisher', 'publishes', 'book'): 4916\n",
            "  - ('user', 'rated', 'book'): 16012\n",
            "  - ('user', 'similar_to', 'user'): 742039\n",
            "\n",
            "Node features:\n",
            "Node type: author\n",
            "  Features: ['nid', 'original_id', 'feat']\n",
            "    - nid: shape torch.Size([2021])\n",
            "    - original_id: shape torch.Size([2021])\n",
            "    - feat: shape torch.Size([2021, 64])\n",
            "Node type: book\n",
            "  Features: ['original_id', 'nid', 'features', 'feat']\n",
            "    - original_id: shape torch.Size([4957])\n",
            "    - nid: shape torch.Size([4957])\n",
            "    - features: shape torch.Size([4957, 3])\n",
            "    - feat: shape torch.Size([4957, 64])\n",
            "Node type: genre\n",
            "  Features: ['nid', 'original_id', 'feat']\n",
            "    - nid: shape torch.Size([165])\n",
            "    - original_id: shape torch.Size([165])\n",
            "    - feat: shape torch.Size([165, 64])\n",
            "Node type: publisher\n",
            "  Features: ['nid', 'original_id', 'feat']\n",
            "    - nid: shape torch.Size([222])\n",
            "    - original_id: shape torch.Size([222])\n",
            "    - feat: shape torch.Size([222, 64])\n",
            "Node type: user\n",
            "  Features: ['nid', 'original_id', 'feat']\n",
            "    - nid: shape torch.Size([44490])\n",
            "    - original_id: shape torch.Size([44490])\n",
            "    - feat: shape torch.Size([44490, 64])\n",
            "\n",
            "Edge features:\n",
            "Edge type: ('author', 'wrote', 'book')\n",
            "Edge type: ('book', 'categorized_as', 'genre')\n",
            "Edge type: ('book', 'published_by', 'publisher')\n",
            "Edge type: ('book', 'rated_by', 'user')\n",
            "    - rating: shape torch.Size([16004])\n",
            "Edge type: ('book', 'written_by', 'author')\n",
            "Edge type: ('genre', 'categorized_in', 'book')\n",
            "Edge type: ('publisher', 'publishes', 'book')\n",
            "Edge type: ('user', 'rated', 'book')\n",
            "    - rating: shape torch.Size([16012])\n",
            "Edge type: ('user', 'similar_to', 'user')\n",
            "\n",
            "\n",
            "\n",
            "Graph Summary: Test Graph\n",
            "=====================\n",
            "Number of nodes:\n",
            "  - author: 2021\n",
            "  - book: 4957\n",
            "  - genre: 165\n",
            "  - publisher: 222\n",
            "  - user: 44490\n",
            "\n",
            "Number of edges:\n",
            "  - ('author', 'wrote', 'book'): 4945\n",
            "  - ('book', 'categorized_as', 'genre'): 12717\n",
            "  - ('book', 'published_by', 'publisher'): 4916\n",
            "  - ('book', 'rated_by', 'user'): 15050\n",
            "  - ('book', 'written_by', 'author'): 4945\n",
            "  - ('genre', 'categorized_in', 'book'): 12717\n",
            "  - ('publisher', 'publishes', 'book'): 4916\n",
            "  - ('user', 'rated', 'book'): 15055\n",
            "  - ('user', 'similar_to', 'user'): 742039\n",
            "\n",
            "Node features:\n",
            "Node type: author\n",
            "  Features: ['nid', 'original_id', 'feat']\n",
            "    - nid: shape torch.Size([2021])\n",
            "    - original_id: shape torch.Size([2021])\n",
            "    - feat: shape torch.Size([2021, 64])\n",
            "Node type: book\n",
            "  Features: ['original_id', 'nid', 'features', 'feat']\n",
            "    - original_id: shape torch.Size([4957])\n",
            "    - nid: shape torch.Size([4957])\n",
            "    - features: shape torch.Size([4957, 3])\n",
            "    - feat: shape torch.Size([4957, 64])\n",
            "Node type: genre\n",
            "  Features: ['nid', 'original_id', 'feat']\n",
            "    - nid: shape torch.Size([165])\n",
            "    - original_id: shape torch.Size([165])\n",
            "    - feat: shape torch.Size([165, 64])\n",
            "Node type: publisher\n",
            "  Features: ['nid', 'original_id', 'feat']\n",
            "    - nid: shape torch.Size([222])\n",
            "    - original_id: shape torch.Size([222])\n",
            "    - feat: shape torch.Size([222, 64])\n",
            "Node type: user\n",
            "  Features: ['nid', 'original_id', 'feat']\n",
            "    - nid: shape torch.Size([44490])\n",
            "    - original_id: shape torch.Size([44490])\n",
            "    - feat: shape torch.Size([44490, 64])\n",
            "\n",
            "Edge features:\n",
            "Edge type: ('author', 'wrote', 'book')\n",
            "Edge type: ('book', 'categorized_as', 'genre')\n",
            "Edge type: ('book', 'published_by', 'publisher')\n",
            "Edge type: ('book', 'rated_by', 'user')\n",
            "    - rating: shape torch.Size([15050])\n",
            "Edge type: ('book', 'written_by', 'author')\n",
            "Edge type: ('genre', 'categorized_in', 'book')\n",
            "Edge type: ('publisher', 'publishes', 'book')\n",
            "Edge type: ('user', 'rated', 'book')\n",
            "    - rating: shape torch.Size([15055])\n",
            "Edge type: ('user', 'similar_to', 'user')\n",
            "\n",
            "\n",
            "تم حفظ الرسوم البيانية الفرعية.\n"
          ]
        }
      ],
      "source": [
        "# -*- coding: utf-8 -*-\n",
        "\"\"\"\n",
        "Graph Pre‑processing & Split\n",
        "=================================================\n",
        "Summary\n",
        "-------\n",
        "* Loads a previously‑built **DGL HeteroGraph** that already contains `original_id`\n",
        "  and `nid` fields (adds them if missing).\n",
        "* Creates **initial node embeddings**: uses existing numeric features when\n",
        "  available or falls back to trainable random embeddings (dimension =`embedding_dim`).\n",
        "* Splits the *user→book* rating edges into **train/validation/test** partitions\n",
        "  **by user**, ensuring that no user appears in more than one split; the split\n",
        "  is mirrored onto the reverse *book→user* edges.\n",
        "* Builds three edge‑masked **sub‑graphs** (train, val, test), prints a compact\n",
        "  summary for each, and saves them to disk for downstream model training.\n",
        "\"\"\"\n",
        "import dgl\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import numpy as np\n",
        "import os\n",
        "from collections import defaultdict\n",
        "\n",
        "# 1. load graph and make sure'original_id' و 'nid'\n",
        "def load_original_graph(filepath):\n",
        "    dgl_graph_list, _ = dgl.load_graphs(filepath)\n",
        "    original_graph = dgl_graph_list[0]\n",
        "\n",
        "    for ntype in original_graph.ntypes:\n",
        "        num_nodes = original_graph.num_nodes(ntype)\n",
        "        if 'original_id' not in original_graph.nodes[ntype].data:\n",
        "            original_graph.nodes[ntype].data['original_id'] = torch.arange(num_nodes)\n",
        "            print(f\"تم إضافة 'original_id' إلى العقد من النوع '{ntype}'.\")\n",
        "        if 'nid' not in original_graph.nodes[ntype].data:\n",
        "            original_graph.nodes[ntype].data['nid'] = torch.arange(num_nodes)\n",
        "            print(f\"تم إضافة 'nid' إلى العقد من النوع '{ntype}'.\")\n",
        "\n",
        "    return original_graph\n",
        "\n",
        "# 2. Create initial embeddings and add them.\n",
        "def create_initial_embeddings(original_graph, embedding_dim=64):\n",
        "    embedding_layers = {}\n",
        "    for ntype in original_graph.ntypes:\n",
        "        num_nodes = original_graph.num_nodes(ntype)\n",
        "        node_data = original_graph.nodes[ntype].data\n",
        "\n",
        "        if 'features' in node_data and len(node_data['features'].shape) == 2:\n",
        "            original_features = node_data['features']\n",
        "            if original_features.shape[1] < embedding_dim:\n",
        "                padding = torch.zeros((num_nodes, embedding_dim - original_features.shape[1]))\n",
        "                expanded_features = torch.cat([original_features, padding], dim=1)\n",
        "                original_graph.nodes[ntype].data['feat'] = expanded_features\n",
        "                print(f\"تم توسيع ميزات العقد من النوع '{ntype}' إلى الأبعاد {embedding_dim}.\")\n",
        "            else:\n",
        "                original_graph.nodes[ntype].data['feat'] = original_features\n",
        "                print(f\"تم الاحتفاظ بالميزات الأصلية للعقد من النوع '{ntype}'.\")\n",
        "        else:\n",
        "            embedding_layer = nn.Embedding(num_nodes, embedding_dim)\n",
        "            embeddings = embedding_layer.weight.data.clone()\n",
        "            original_graph.nodes[ntype].data['feat'] = embeddings\n",
        "            embedding_layers[ntype] = embedding_layer\n",
        "            print(f\"تم إنشاء تضمينات عشوائية قابلة للتدريب للعقد من النوع '{ntype}'.\")\n",
        "            if torch.all(embeddings == 0):\n",
        "                print(f\"تحذير: تم العثور على تضمينات صفريّة للعقد من النوع '{ntype}'. إعادة إنشاء التضمينات.\")\n",
        "                embeddings.uniform_(-0.1, 0.1)\n",
        "                original_graph.nodes[ntype].data['feat'] = embeddings\n",
        "\n",
        "    return embedding_layers\n",
        "\n",
        "# 3. Split edges of rated و rated_by\n",
        "def split_edges_by_users(graph, train_ratio=0.8, val_ratio=0.1):\n",
        "    # Extract edges from type'rated'\n",
        "    rated_src, rated_dst = graph.edges(etype=('user', 'rated', 'book'))\n",
        "\n",
        "    # Edge aggregation for each user\n",
        "    user_edges = defaultdict(list)\n",
        "    for idx, src in enumerate(rated_src.tolist()):\n",
        "        user_edges[src].append(idx)\n",
        "\n",
        "    # Divide users into three groups (training, verification, testing)\n",
        "    all_users = list(user_edges.keys())\n",
        "    np.random.shuffle(all_users)  # Randomize users\n",
        "\n",
        "    num_users = len(all_users)\n",
        "    train_end = int(num_users * train_ratio)\n",
        "    val_end = int(num_users * (train_ratio + val_ratio))\n",
        "\n",
        "    train_users = all_users[:train_end]\n",
        "    val_users = all_users[train_end:val_end]\n",
        "    test_users = all_users[val_end:]\n",
        "\n",
        "    # Setting up masks based on user segmentation\n",
        "    num_edges = len(rated_src)\n",
        "    train_mask_rated = torch.zeros(num_edges, dtype=torch.bool)\n",
        "    val_mask_rated = torch.zeros(num_edges, dtype=torch.bool)\n",
        "    test_mask_rated = torch.zeros(num_edges, dtype=torch.bool)\n",
        "\n",
        "    # Assign edges to masks for each group.\n",
        "    for user in train_users:\n",
        "        for edge_idx in user_edges[user]:\n",
        "            train_mask_rated[edge_idx] = True\n",
        "\n",
        "    for user in val_users:\n",
        "        for edge_idx in user_edges[user]:\n",
        "            val_mask_rated[edge_idx] = True\n",
        "\n",
        "    for user in test_users:\n",
        "        for edge_idx in user_edges[user]:\n",
        "            test_mask_rated[edge_idx] = True\n",
        "\n",
        "    # Ensure there is no overlap between edges.\n",
        "    def check_overlap(mask1, mask2):\n",
        "        return (mask1 & mask2).sum().item()\n",
        "\n",
        "    if check_overlap(train_mask_rated, val_mask_rated) > 0 or check_overlap(train_mask_rated, test_mask_rated) > 0:\n",
        "        print(\"تحذير: تداخل بين حواف التدريب والتحقق أو التدريب والاختبار.\")\n",
        "    if check_overlap(val_mask_rated, test_mask_rated) > 0:\n",
        "        print(\"تحذير: تداخل بين حواف التحقق والاختبار.\")\n",
        "\n",
        "    # Create edge lists as ordered pairs\n",
        "    rated_edges = list(zip(rated_src.tolist(), rated_dst.tolist()))\n",
        "    rated_by_src, rated_by_dst = graph.edges(etype=('book', 'rated_by', 'user'))\n",
        "    rated_by_edges = list(zip(rated_by_dst.tolist(), rated_by_src.tolist()))  # (user, book)\n",
        "\n",
        "    # Create maps from pairs to edge indexes\n",
        "    rated_edge_to_index = {edge: idx for idx, edge in enumerate(rated_edges)}\n",
        "    rated_by_edge_to_index = {edge: idx for idx, edge in enumerate(rated_by_edges)}\n",
        "\n",
        "    # Create a map between 'rated' and 'rated_by' edge indexes\n",
        "    rated_to_rated_by_edge_index = {}\n",
        "    for edge, idx in rated_edge_to_index.items():\n",
        "        if edge in rated_by_edge_to_index:\n",
        "            rated_by_idx = rated_by_edge_to_index[edge]\n",
        "            rated_to_rated_by_edge_index[idx] = rated_by_idx\n",
        "\n",
        "    # Create masks for 'rated_by' edges\n",
        "    train_mask_rated_by = torch.zeros(len(rated_by_src), dtype=torch.bool)\n",
        "    val_mask_rated_by = torch.zeros(len(rated_by_src), dtype=torch.bool)\n",
        "    test_mask_rated_by = torch.zeros(len(rated_by_src), dtype=torch.bool)\n",
        "\n",
        "    # Set masks for 'rated_by' edges based on masks for 'rated' edges\n",
        "    for rated_edge_idx, rated_by_edge_idx in rated_to_rated_by_edge_index.items():\n",
        "        if train_mask_rated[rated_edge_idx]:\n",
        "            train_mask_rated_by[rated_by_edge_idx] = True\n",
        "        elif val_mask_rated[rated_edge_idx]:\n",
        "            val_mask_rated_by[rated_by_edge_idx] = True\n",
        "        elif test_mask_rated[rated_edge_idx]:\n",
        "            test_mask_rated_by[rated_by_edge_idx] = True\n",
        "\n",
        "    return (train_mask_rated, val_mask_rated, test_mask_rated,\n",
        "            train_mask_rated_by, val_mask_rated_by, test_mask_rated_by)\n",
        "\n",
        "# 4  create_subgraphs: materialise edge‑masked train/val/test graphs\n",
        "def create_subgraphs(g: dgl.DGLHeteroGraph,\n",
        "                     train_m, val_m, test_m,\n",
        "                     train_rev_m, val_rev_m, test_rev_m):\n",
        "    \"\"\"Return three DGL sub‑graphs using the provided boolean edge masks.\"\"\"\n",
        "    train_g = dgl.edge_subgraph(g, {(\"user\", \"rated\", \"book\"): train_m,\n",
        "                                    (\"book\", \"rated_by\", \"user\"): train_rev_m},\n",
        "                                preserve_nodes=True)\n",
        "    val_g = dgl.edge_subgraph(g, {(\"user\", \"rated\", \"book\"): val_m,\n",
        "                                  (\"book\", \"rated_by\", \"user\"): val_rev_m},\n",
        "                              preserve_nodes=True)\n",
        "    test_g = dgl.edge_subgraph(g, {(\"user\", \"rated\", \"book\"): test_m,\n",
        "                                   (\"book\", \"rated_by\", \"user\"): test_rev_m},\n",
        "                               preserve_nodes=True)\n",
        "    return train_g, val_g, test_g\n",
        "\n",
        "# 5. Print statistics for each subgraph\n",
        "def inspect_graph(graph, name):\n",
        "    print(f\"\\nGraph Summary: {name}\")\n",
        "    print(\"=====================\")\n",
        "    print(f\"Number of nodes:\")\n",
        "    for ntype in graph.ntypes:\n",
        "        print(f\"  - {ntype}: {graph.num_nodes(ntype)}\")\n",
        "    print(f\"\\nNumber of edges:\")\n",
        "    for etype in graph.canonical_etypes:\n",
        "        print(f\"  - {etype}: {graph.num_edges(etype)}\")\n",
        "    print(\"\\nNode features:\")\n",
        "    for ntype in graph.ntypes:\n",
        "        print(f\"Node type: {ntype}\")\n",
        "        node_data_keys = list(graph.nodes[ntype].data.keys())\n",
        "        print(f\"  Features: {node_data_keys}\")\n",
        "        for key in node_data_keys:\n",
        "            data_shape = graph.nodes[ntype].data[key].shape\n",
        "            print(f\"    - {key}: shape {data_shape}\")\n",
        "    print(\"\\nEdge features:\")\n",
        "    for etype in graph.canonical_etypes:\n",
        "        print(f\"Edge type: {etype}\")\n",
        "        edge_data_keys = list(graph.edges[etype].data.keys())\n",
        "        for key in edge_data_keys:\n",
        "            data_shape = graph.edges[etype].data[key].shape\n",
        "            print(f\"    - {key}: shape {data_shape}\")\n",
        "    print(\"\\n\")\n",
        "\n",
        "# 6. Save subgraphs\n",
        "def save_subgraphs(train_graph, val_graph, test_graph, output_dir):\n",
        "    dgl.save_graphs(f\"{output_dir}/train_graph.bin\", [train_graph])\n",
        "    dgl.save_graphs(f\"{output_dir}/val_graph.bin\", [val_graph])\n",
        "    dgl.save_graphs(f\"{output_dir}/test_graph.bin\", [test_graph])\n",
        "    print(\"تم حفظ الرسوم البيانية الفرعية.\")\n",
        "\n",
        "# 7. Complete the process\n",
        "def main():\n",
        "    original_graph_path = '/content/drive/MyDrive/graph/heterogeneous_graph_bert.bin'\n",
        "    output_dir = '/content/drive/MyDrive/graph'\n",
        "    os.makedirs(output_dir, exist_ok=True)\n",
        "\n",
        "    original_graph = load_original_graph(original_graph_path)\n",
        "    embedding_dim = 64\n",
        "    create_initial_embeddings(original_graph, embedding_dim=embedding_dim)\n",
        "\n",
        "    (train_mask_rated, val_mask_rated, test_mask_rated,\n",
        "     train_mask_rated_by, val_mask_rated_by, test_mask_rated_by) = split_edges_by_users(original_graph)\n",
        "\n",
        "    train_graph, val_graph, test_graph = create_subgraphs(\n",
        "        original_graph,\n",
        "        train_mask_rated, val_mask_rated, test_mask_rated,\n",
        "        train_mask_rated_by, val_mask_rated_by, test_mask_rated_by\n",
        "    )\n",
        "\n",
        "    inspect_graph(train_graph, \"Training Graph\")\n",
        "    inspect_graph(val_graph, \"Validation Graph\")\n",
        "    inspect_graph(test_graph, \"Test Graph\")\n",
        "\n",
        "    save_subgraphs(train_graph, val_graph, test_graph, output_dir)\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_9HAoQjKu3br",
        "outputId": "0b27ed40-a882-40d4-905b-b6d1cdc5fba5"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "لا يوجد تداخل بين حواف التدريب والتحقق.\n",
            "لا يوجد تداخل بين حواف التدريب والاختبار.\n",
            "لا يوجد تداخل بين حواف التحقق والاختبار.\n"
          ]
        }
      ],
      "source": [
        "# -*- coding: utf-8 -*-\n",
        "\"\"\"\n",
        "Edge‑Overlap Sanity Check –\n",
        "====================================\n",
        "Summary\n",
        "-------\n",
        "This utility script\n",
        "1. **Loads** the three edge‑masked sub‑graphs (train / validation / test)\n",
        "   you previously saved with `save_subgraphs`.\n",
        "2. **Extracts** the `(user, book)` pairs for the forward rating relation\n",
        "   `(user, rated, book)` from each split.\n",
        "3. **Calculates intersections** between the three edge sets to confirm that no\n",
        "   interaction leaks across splits – a critical guarantee for unbiased model\n",
        "   evaluation.\n",
        "4. **Prints a concise report** listing any overlaps found (ideally zero).\n",
        "\n",
        "\"\"\"\n",
        "import dgl\n",
        "import torch\n",
        "\n",
        "def load_subgraphs(output_dir):\n",
        "    # load subgraphs for training, validation, and testing.\n",
        "    train_graph = dgl.load_graphs(f\"{output_dir}/train_graph.bin\")[0][0]\n",
        "    val_graph = dgl.load_graphs(f\"{output_dir}/val_graph.bin\")[0][0]\n",
        "    test_graph = dgl.load_graphs(f\"{output_dir}/test_graph.bin\")[0][0]\n",
        "    return train_graph, val_graph, test_graph\n",
        "\n",
        "def check_edge_overlap(train_graph, val_graph, test_graph):\n",
        "    # Extract edges from graphs for each group (training, validation, testing)\n",
        "    train_edges = set(zip(*train_graph.edges(etype=('user', 'rated', 'book'))))\n",
        "    val_edges = set(zip(*val_graph.edges(etype=('user', 'rated', 'book'))))\n",
        "    test_edges = set(zip(*test_graph.edges(etype=('user', 'rated', 'book'))))\n",
        "\n",
        "    # Checking overlap between training and validation edges\n",
        "    train_val_overlap = train_edges.intersection(val_edges)\n",
        "    train_test_overlap = train_edges.intersection(test_edges)\n",
        "    val_test_overlap = val_edges.intersection(test_edges)\n",
        "\n",
        "    # Show results\n",
        "    if train_val_overlap:\n",
        "        print(f\"تداخل بين حواف التدريب والتحقق: {len(train_val_overlap)} حافة.\")\n",
        "    else:\n",
        "        print(\"لا يوجد تداخل بين حواف التدريب والتحقق.\")\n",
        "\n",
        "    if train_test_overlap:\n",
        "        print(f\"تداخل بين حواف التدريب والاختبار: {len(train_test_overlap)} حافة.\")\n",
        "    else:\n",
        "        print(\"لا يوجد تداخل بين حواف التدريب والاختبار.\")\n",
        "\n",
        "    if val_test_overlap:\n",
        "        print(f\"تداخل بين حواف التحقق والاختبار: {len(val_test_overlap)} حافة.\")\n",
        "    else:\n",
        "        print(\"لا يوجد تداخل بين حواف التحقق والاختبار.\")\n",
        "\n",
        "# Path of the folder containing the subgraphs\n",
        "output_dir = '/content/drive/MyDrive/graph'\n",
        "\n",
        "# Load subgraphs and check for overlap.\n",
        "train_graph, val_graph, test_graph = load_subgraphs(output_dir)\n",
        "check_edge_overlap(train_graph, val_graph, test_graph)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yjo2Nm8-q_17",
        "outputId": "2c57a710-310a-4f1d-9dc1-48b93f82116b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Processing metapath: author_path\n",
            "Saved training subgraph for author_path at /content/drive/MyDrive/graph/metapath_subgraphs/train_subgraph_author_path.bin\n",
            "Saved validation subgraph for author_path at /content/drive/MyDrive/graph/metapath_subgraphs/val_subgraph_author_path.bin\n",
            "Saved test subgraph for author_path at /content/drive/MyDrive/graph/metapath_subgraphs/test_subgraph_author_path.bin\n",
            "Processing metapath: genre_path\n",
            "Saved training subgraph for genre_path at /content/drive/MyDrive/graph/metapath_subgraphs/train_subgraph_genre_path.bin\n",
            "Saved validation subgraph for genre_path at /content/drive/MyDrive/graph/metapath_subgraphs/val_subgraph_genre_path.bin\n",
            "Saved test subgraph for genre_path at /content/drive/MyDrive/graph/metapath_subgraphs/test_subgraph_genre_path.bin\n",
            "Processing metapath: similar_path\n",
            "Saved training subgraph for similar_path at /content/drive/MyDrive/graph/metapath_subgraphs/train_subgraph_similar_path.bin\n",
            "Saved validation subgraph for similar_path at /content/drive/MyDrive/graph/metapath_subgraphs/val_subgraph_similar_path.bin\n",
            "Saved test subgraph for similar_path at /content/drive/MyDrive/graph/metapath_subgraphs/test_subgraph_similar_path.bin\n"
          ]
        }
      ],
      "source": [
        "# -*- coding: utf-8 -*-\n",
        "\"\"\"\n",
        "Metapath-Based Subgraph Extraction\n",
        "==================================\n",
        "Summary\n",
        "-------\n",
        "This script processes previously generated training, validation, and test graphs by:\n",
        "\n",
        "1. Loading the pre-saved subgraphs from disk.\n",
        "2. Defining specific metapaths (semantic paths) such as author-path, genre-path, and similarity-path.\n",
        "3. Extracting new subgraphs based explicitly on these metapaths while preserving original node and edge features.\n",
        "4. Saving these metapath-specific subgraphs separately for each dataset split (train, validation, test).\n",
        "\"\"\"\n",
        "import dgl\n",
        "import torch\n",
        "import os\n",
        "\n",
        "# 1. load subgraphs for training, validation, and testing.\n",
        "def load_graphs(base_dir):\n",
        "    train_graph = dgl.load_graphs(os.path.join(base_dir, 'train_graph.bin'))[0][0]\n",
        "    val_graph = dgl.load_graphs(os.path.join(base_dir, 'val_graph.bin'))[0][0]\n",
        "    test_graph = dgl.load_graphs(os.path.join(base_dir, 'test_graph.bin'))[0][0]\n",
        "    return train_graph, val_graph, test_graph\n",
        "\n",
        "# 2. Definition of meta_paths\n",
        "def define_metapaths():\n",
        "    metapaths = {\n",
        "        'author_path': [\n",
        "            ('user', 'rated', 'book'),\n",
        "            ('book', 'written_by', 'author'),\n",
        "            ('author', 'wrote','book' ),\n",
        "            ('book', 'rated_by', 'user')\n",
        "\n",
        "        ],\n",
        "        'genre_path': [\n",
        "            ('user', 'rated', 'book'),\n",
        "            ('book', 'categorized_as', 'genre'),\n",
        "            ('genre', 'categorized_in', 'book'),\n",
        "            ('book', 'rated_by', 'user')\n",
        "        ],\n",
        "        'similar_path': [\n",
        "            ('user', 'similar_to', 'user'),\n",
        "            ('user', 'rated', 'book')\n",
        "        ]\n",
        "    }\n",
        "    return metapaths\n",
        "\n",
        "# 3. Extract subgraphs based on path\n",
        "def extract_metapath_subgraph(graph, metapath):\n",
        "    # Extract edge types from a path\n",
        "    edge_types = list(set([ (src, etype, dst) for src, etype, dst in metapath ]))\n",
        "    # Create a subgraph\n",
        "    subgraph = graph.edge_type_subgraph(edge_types)\n",
        "\n",
        "    # Copy features from the original graph to the subgraph\n",
        "    for ntype in subgraph.ntypes:\n",
        "        for feature in graph.nodes[ntype].data.keys():\n",
        "            subgraph.nodes[ntype].data[feature] = graph.nodes[ntype].data[feature]\n",
        "\n",
        "    for etype in subgraph.canonical_etypes:\n",
        "        for feature in graph.edges[etype].data.keys():\n",
        "            subgraph.edges[etype].data[feature] = graph.edges[etype].data[feature]\n",
        "\n",
        "    return subgraph\n",
        "\n",
        "# 4. Save subgraphs\n",
        "def save_subgraph(subgraph, filepath):\n",
        "    dgl.save_graphs(filepath, [subgraph])\n",
        "\n",
        "# 5. Process each path and dataset\n",
        "def process_metapaths(train_graph, val_graph, test_graph, metapaths, output_dir):\n",
        "    for metapath_name, metapath in metapaths.items():\n",
        "        print(f\"Processing metapath: {metapath_name}\")\n",
        "\n",
        "        #Extract subgraphs for training\n",
        "        train_subgraph = extract_metapath_subgraph(train_graph, metapath)\n",
        "        if train_subgraph.num_edges() > 0:\n",
        "            train_subgraph_path = os.path.join(output_dir, f\"train_subgraph_{metapath_name}.bin\")\n",
        "            save_subgraph(train_subgraph, train_subgraph_path)\n",
        "            print(f\"Saved training subgraph for {metapath_name} at {train_subgraph_path}\")\n",
        "        else:\n",
        "            print(f\"No edges found for training subgraph of {metapath_name}.\")\n",
        "\n",
        "        # Extract subgraphs for verification\n",
        "        val_subgraph = extract_metapath_subgraph(val_graph, metapath)\n",
        "        if val_subgraph.num_edges() > 0:\n",
        "            val_subgraph_path = os.path.join(output_dir, f\"val_subgraph_{metapath_name}.bin\")\n",
        "            save_subgraph(val_subgraph, val_subgraph_path)\n",
        "            print(f\"Saved validation subgraph for {metapath_name} at {val_subgraph_path}\")\n",
        "        else:\n",
        "            print(f\"No edges found for validation subgraph of {metapath_name}.\")\n",
        "\n",
        "        # Extract subgraphs for testing\n",
        "        test_subgraph = extract_metapath_subgraph(test_graph, metapath)\n",
        "        if test_subgraph.num_edges() > 0:\n",
        "            test_subgraph_path = os.path.join(output_dir, f\"test_subgraph_{metapath_name}.bin\")\n",
        "            save_subgraph(test_subgraph, test_subgraph_path)\n",
        "            print(f\"Saved test subgraph for {metapath_name} at {test_subgraph_path}\")\n",
        "        else:\n",
        "            print(f\"No edges found for test subgraph of {metapath_name}.\")\n",
        "\n",
        "def main():\n",
        "    # paths\n",
        "    base_dir = '/content/drive/MyDrive/graph'  # المسار الذي يحتوي على الرسوم البيانية الفرعية\n",
        "    output_dir = '/content/drive/MyDrive/graph/metapath_subgraphs'  # المسار لحفظ الرسوم البيانية الفرعية للمسارات\n",
        "    os.makedirs(output_dir, exist_ok=True)\n",
        "\n",
        "    # 1. load graphs for training, validation, and testing.\n",
        "    train_graph, val_graph, test_graph = load_graphs(base_dir)\n",
        "\n",
        "    # 2. processing and subgraph extraction\n",
        "    metapaths = define_metapaths()\n",
        "    process_metapaths(train_graph, val_graph, test_graph, metapaths, output_dir)\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dq0QvidChakT",
        "outputId": "de29516a-c17c-41ba-c29c-ead81abb9a13"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "[INFO] Graph: Main Graph\n",
            "Number of nodes: 51855\n",
            "Number of edges: 1100195\n",
            "\tNode type 'author': 2021 nodes\n",
            "\tNode type 'book': 4957 nodes\n",
            "\tNode type 'genre': 165 nodes\n",
            "\tNode type 'publisher': 222 nodes\n",
            "\tNode type 'user': 44490 nodes\n",
            "\tEdge type 'wrote': 4945 edges\n",
            "\tEdge type 'categorized_as': 12717 edges\n",
            "\tEdge type 'published_by': 4916 edges\n",
            "\tEdge type 'rated_by': 156500 edges\n",
            "\tEdge type 'written_by': 4945 edges\n",
            "\tEdge type 'categorized_in': 12717 edges\n",
            "\tEdge type 'publishes': 4916 edges\n",
            "\tEdge type 'rated': 156500 edges\n",
            "\tEdge type 'similar_to': 742039 edges\n",
            "\n",
            "[INFO] Graph: Train Subgraph 1\n",
            "Number of nodes: 49612\n",
            "Number of edges: 276228\n",
            "\tNode type 'book': 4957 nodes\n",
            "\tNode type 'genre': 165 nodes\n",
            "\tNode type 'user': 44490 nodes\n",
            "\tEdge type 'rated': 125433 edges\n",
            "\tEdge type 'categorized_in': 12717 edges\n",
            "\tEdge type 'categorized_as': 12717 edges\n",
            "\tEdge type 'rated_by': 125361 edges\n",
            "\n",
            "[INFO] Graph: Train Subgraph 2\n",
            "Number of nodes: 51468\n",
            "Number of edges: 260684\n",
            "\tNode type 'author': 2021 nodes\n",
            "\tNode type 'book': 4957 nodes\n",
            "\tNode type 'user': 44490 nodes\n",
            "\tEdge type 'rated': 125433 edges\n",
            "\tEdge type 'wrote': 4945 edges\n",
            "\tEdge type 'written_by': 4945 edges\n",
            "\tEdge type 'rated_by': 125361 edges\n",
            "\n",
            "[INFO] Graph: Train Subgraph 3\n",
            "Number of nodes: 49447\n",
            "Number of edges: 867472\n",
            "\tNode type 'book': 4957 nodes\n",
            "\tNode type 'user': 44490 nodes\n",
            "\tEdge type 'rated': 125433 edges\n",
            "\tEdge type 'similar_to': 742039 edges\n",
            "\n",
            "[INFO] Graph: Validation Subgraph 1\n",
            "Number of nodes: 49612\n",
            "Number of edges: 57450\n",
            "\tNode type 'book': 4957 nodes\n",
            "\tNode type 'genre': 165 nodes\n",
            "\tNode type 'user': 44490 nodes\n",
            "\tEdge type 'rated': 16012 edges\n",
            "\tEdge type 'categorized_in': 12717 edges\n",
            "\tEdge type 'categorized_as': 12717 edges\n",
            "\tEdge type 'rated_by': 16004 edges\n",
            "\n",
            "[INFO] Graph: Validation Subgraph 2\n",
            "Number of nodes: 51468\n",
            "Number of edges: 41906\n",
            "\tNode type 'author': 2021 nodes\n",
            "\tNode type 'book': 4957 nodes\n",
            "\tNode type 'user': 44490 nodes\n",
            "\tEdge type 'rated': 16012 edges\n",
            "\tEdge type 'wrote': 4945 edges\n",
            "\tEdge type 'written_by': 4945 edges\n",
            "\tEdge type 'rated_by': 16004 edges\n",
            "\n",
            "[INFO] Graph: Validation Subgraph 3\n",
            "Number of nodes: 49447\n",
            "Number of edges: 758051\n",
            "\tNode type 'book': 4957 nodes\n",
            "\tNode type 'user': 44490 nodes\n",
            "\tEdge type 'rated': 16012 edges\n",
            "\tEdge type 'similar_to': 742039 edges\n",
            "\n",
            "[INFO] Graph: Test Subgraph 1\n",
            "Number of nodes: 49612\n",
            "Number of edges: 55539\n",
            "\tNode type 'book': 4957 nodes\n",
            "\tNode type 'genre': 165 nodes\n",
            "\tNode type 'user': 44490 nodes\n",
            "\tEdge type 'rated': 15055 edges\n",
            "\tEdge type 'categorized_in': 12717 edges\n",
            "\tEdge type 'categorized_as': 12717 edges\n",
            "\tEdge type 'rated_by': 15050 edges\n",
            "\n",
            "[INFO] Graph: Test Subgraph 2\n",
            "Number of nodes: 51468\n",
            "Number of edges: 39995\n",
            "\tNode type 'author': 2021 nodes\n",
            "\tNode type 'book': 4957 nodes\n",
            "\tNode type 'user': 44490 nodes\n",
            "\tEdge type 'rated': 15055 edges\n",
            "\tEdge type 'wrote': 4945 edges\n",
            "\tEdge type 'written_by': 4945 edges\n",
            "\tEdge type 'rated_by': 15050 edges\n",
            "\n",
            "[INFO] Graph: Test Subgraph 3\n",
            "Number of nodes: 49447\n",
            "Number of edges: 757094\n",
            "\tNode type 'book': 4957 nodes\n",
            "\tNode type 'user': 44490 nodes\n",
            "\tEdge type 'rated': 15055 edges\n",
            "\tEdge type 'similar_to': 742039 edges\n"
          ]
        }
      ],
      "source": [
        "# -*- coding: utf-8 -*-\n",
        "\"\"\"\n",
        "Graph Statistics and Overview\n",
        "============================\n",
        "Summary\n",
        "-------\n",
        "This script loads a main heterogeneous graph and multiple subgraphs generated based on specific semantic metapaths (genre-based, author-based, similarity-based) for the train, validation, and test splits. It then prints concise statistics for each loaded graph, including:\n",
        "\n",
        "- Total number of nodes and edges.\n",
        "- Counts of nodes and edges categorized by type.\n",
        "\n",
        "\"\"\"\n",
        "import dgl\n",
        "\n",
        "# Load graphs\n",
        "def load_graph(file_path):\n",
        "    graphs, _ = dgl.load_graphs(file_path)\n",
        "    return graphs[0]\n",
        "\n",
        "# Function to print graph statistics\n",
        "def print_graph_statistics(graph, graph_name):\n",
        "    print(f\"\\n[INFO] Graph: {graph_name}\")\n",
        "    print(f\"Number of nodes: {graph.number_of_nodes()}\")\n",
        "    print(f\"Number of edges: {graph.number_of_edges()}\")\n",
        "    for ntype in graph.ntypes:\n",
        "        print(f\"\\tNode type '{ntype}': {graph.num_nodes(ntype)} nodes\")\n",
        "    for etype in graph.etypes:\n",
        "        print(f\"\\tEdge type '{etype}': {graph.num_edges(etype)} edges\")\n",
        "\n",
        "\n",
        "main_graph_path = '/content/drive/MyDrive/graph/heterogeneous_graph_bert.bin'\n",
        "train_subgraphs_paths = [\n",
        "    '/content/drive/MyDrive/graph/metapath_subgraphs/train_subgraph_genre_path.bin',\n",
        "    '/content/drive/MyDrive/graph/metapath_subgraphs/train_subgraph_author_path.bin',\n",
        "    '/content/drive/MyDrive/graph/metapath_subgraphs/train_subgraph_similar_path.bin'\n",
        "]\n",
        "val_subgraphs_paths = [\n",
        "    '/content/drive/MyDrive/graph/metapath_subgraphs/val_subgraph_genre_path.bin',\n",
        "    '/content/drive/MyDrive/graph/metapath_subgraphs/val_subgraph_author_path.bin',\n",
        "    '/content/drive/MyDrive/graph/metapath_subgraphs/val_subgraph_similar_path.bin'\n",
        "]\n",
        "test_subgraphs_paths = [\n",
        "    '/content/drive/MyDrive/graph/metapath_subgraphs/test_subgraph_genre_path.bin',\n",
        "    '/content/drive/MyDrive/graph/metapath_subgraphs/test_subgraph_author_path.bin',\n",
        "    '/content/drive/MyDrive/graph/metapath_subgraphs/test_subgraph_similar_path.bin'\n",
        "]\n",
        "\n",
        "# load and print main graph statistics\n",
        "main_graph = load_graph(main_graph_path)\n",
        "print_graph_statistics(main_graph, \"Main Graph\")\n",
        "\n",
        "# # load and print traing graph statistics\n",
        "for i, path in enumerate(train_subgraphs_paths):\n",
        "    train_graph = load_graph(path)\n",
        "    print_graph_statistics(train_graph, f\"Train Subgraph {i + 1}\")\n",
        "\n",
        "# # load and print validation graph statistics\n",
        "for i, path in enumerate(val_subgraphs_paths):\n",
        "    val_graph = load_graph(path)\n",
        "    print_graph_statistics(val_graph, f\"Validation Subgraph {i + 1}\")\n",
        "\n",
        "# # load and print test graph statistics\n",
        "for i, path in enumerate(test_subgraphs_paths):\n",
        "    test_graph = load_graph(path)\n",
        "    print_graph_statistics(test_graph, f\"Test Subgraph {i + 1}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "smq5lxoFu3YB",
        "outputId": "0ecda4df-d986-40b8-8b6e-1db410b86548"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loaded subgraph 1 from /content/drive/MyDrive/graph/metapath_subgraphs/train_subgraph_genre_path.bin\n",
            "Loaded subgraph 2 from /content/drive/MyDrive/graph/metapath_subgraphs/train_subgraph_author_path.bin\n",
            "Loaded subgraph 3 from /content/drive/MyDrive/graph/metapath_subgraphs/train_subgraph_similar_path.bin\n",
            "Loaded subgraph 1 from /content/drive/MyDrive/graph/metapath_subgraphs/val_subgraph_genre_path.bin\n",
            "Loaded subgraph 2 from /content/drive/MyDrive/graph/metapath_subgraphs/val_subgraph_author_path.bin\n",
            "Loaded subgraph 3 from /content/drive/MyDrive/graph/metapath_subgraphs/val_subgraph_similar_path.bin\n",
            "Initialized GAT models with Xavier initialization.\n",
            "Initialized GAT models with Xavier initialization.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/dgl/backend/pytorch/tensor.py:445: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
            "  assert input.numel() == input.storage().size(), (\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Epoch 01] Training Loss: 0.692923\n",
            "[Epoch 01] Validation Loss: 0.693462\n",
            "[Epoch 02] Training Loss: 0.692534\n",
            "[Epoch 02] Validation Loss: 0.693206\n",
            "[Epoch 03] Training Loss: 0.692308\n",
            "[Epoch 03] Validation Loss: 0.693405\n",
            "[Epoch 04] Training Loss: 0.692124\n",
            "[Epoch 04] Validation Loss: 0.693531\n",
            "[Epoch 05] Training Loss: 0.692035\n",
            "[Epoch 05] Validation Loss: 0.693433\n",
            "Early stopping triggered!\n",
            "Best Model Validation Loss: 0.693206\n"
          ]
        }
      ],
      "source": [
        "# -*- coding: utf-8 -*-\n",
        "\"\"\"\n",
        "Traing Graph Attention Network (GAT) with MLP Using Mean -based Attention Aggregation\n",
        "========================================================================\n",
        "Summary\n",
        "-------\n",
        "This comprehensive script:\n",
        "1. **Loads metapath-based training and validation subgraphs** previously generated and saved to disk.\n",
        "2. Defines and initializes multiple **Graph Attention Network (GAT)** models, one per semantic metapath (author-based, genre-based, similarity-based).\n",
        "3. Implements a custom GAT convolution layer to store and access attention weights for interpretability.\n",
        "4. Defines a **Multi-Layer Perceptron (MLP)** model without a sigmoid layer (for compatibility with BCEWithLogitsLoss).\n",
        "5. Conducts a robust **training and validation loop** for rating prediction tasks, ensuring model convergence with early stopping.\n",
        "6. Saves the best-performing MLP model and the trained GAT models.\n",
        "\"\"\"\n",
        "import dgl\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import numpy as np\n",
        "import dgl.nn as dglnn\n",
        "from collections import defaultdict\n",
        "\n",
        "# Load subgraphs for training and validating.\n",
        "def load_subgraphs(paths):\n",
        "    subgraphs = [dgl.load_graphs(path)[0][0] for path in paths]\n",
        "    for i, subgraph in enumerate(subgraphs):\n",
        "        print(f\"Loaded subgraph {i+1} from {paths[i]}\")\n",
        "    return subgraphs\n",
        "\n",
        "\n",
        "train_subgraph_paths = [\n",
        "    '/content/drive/MyDrive/graph/metapath_subgraphs/train_subgraph_genre_path.bin',\n",
        "    '/content/drive/MyDrive/graph/metapath_subgraphs/train_subgraph_author_path.bin',\n",
        "    '/content/drive/MyDrive/graph/metapath_subgraphs/train_subgraph_similar_path.bin'\n",
        "]\n",
        "\n",
        "val_subgraph_paths = [\n",
        "    '/content/drive/MyDrive/graph/metapath_subgraphs/val_subgraph_genre_path.bin',\n",
        "    '/content/drive/MyDrive/graph/metapath_subgraphs/val_subgraph_author_path.bin',\n",
        "    '/content/drive/MyDrive/graph/metapath_subgraphs/val_subgraph_similar_path.bin'\n",
        "]\n",
        "\n",
        "train_meta_path_subgraphs = load_subgraphs(train_subgraph_paths)\n",
        "val_meta_path_subgraphs = load_subgraphs(val_subgraph_paths)\n",
        "\n",
        "# Define a custom layer for GATConv to store attention weights\n",
        "class CustomGATConv(dglnn.GATConv):\n",
        "    def forward(self, graph, feat):\n",
        "        # Perform forward pass of GATConv and get attention weights\n",
        "        h, attn_weights = super().forward(graph, feat, get_attention=True)\n",
        "\n",
        "        self.attention_weights = attn_weights\n",
        "        return h\n",
        "\n",
        "#Defining a GAT model using `CustomGATConv`\n",
        "class MetaPathGAT(nn.Module):\n",
        "    def __init__(self, hidden_dim, out_dim, num_heads, etypes, ntypes, layer_structure, dropout):\n",
        "        super(MetaPathGAT, self).__init__()\n",
        "        self.hidden_dim = hidden_dim\n",
        "        self.num_heads = num_heads\n",
        "        self.etypes = etypes\n",
        "        self.layer_structure = layer_structure\n",
        "\n",
        "        # تعريف طبقات GAT لكل نوع حافة باستخدام `CustomGATConv`\n",
        "        self.gat_layers = nn.ModuleDict({\n",
        "            str(etype): nn.ModuleList([\n",
        "                CustomGATConv(\n",
        "                    in_feats=hidden_dim,\n",
        "                    out_feats=hidden_dim // num_heads,\n",
        "                    num_heads=num_heads,\n",
        "                    feat_drop=dropout,\n",
        "                    attn_drop=dropout,\n",
        "                    activation=nn.ELU(),\n",
        "                    allow_zero_in_degree=True\n",
        "                )\n",
        "                for _ in range(len(layer_structure))\n",
        "            ]) for etype in etypes\n",
        "        })\n",
        "\n",
        "        # طبقات Fully connected لكل نوع عقدة\n",
        "        self.fc = nn.ModuleDict({\n",
        "            ntype: nn.Linear(hidden_dim // num_heads, out_dim) for ntype in ntypes\n",
        "        })\n",
        "\n",
        "        # Dictionary of Attention Weights\n",
        "        self.attention_weights = defaultdict(list)\n",
        "\n",
        "    def forward(self, g, features):\n",
        "        h_dict = {}\n",
        "        self.attention_weights.clear()  # Clear previous weights on each pass\n",
        "        for etype in g.canonical_etypes:\n",
        "            src_type, edge_type, dst_type = etype\n",
        "            etype_key = str(etype)\n",
        "\n",
        "            if etype_key not in self.gat_layers:\n",
        "                continue\n",
        "            if (src_type not in features) or (dst_type not in features):\n",
        "                continue\n",
        "\n",
        "            h_src = features[src_type]\n",
        "            h_dst = features[dst_type] if src_type != dst_type else h_src\n",
        "\n",
        "            # Apply GAT layers and save attention weights\n",
        "            for layer in self.gat_layers[etype_key]:\n",
        "                h = layer(g[etype], (h_src, h_dst))\n",
        "                # save attention weights\n",
        "                self.attention_weights[etype_key].append(layer.attention_weights)\n",
        "\n",
        "            # Average head attention\n",
        "            h = h.mean(dim=1)\n",
        "\n",
        "            if dst_type in h_dict:\n",
        "                h_dict[dst_type] += h\n",
        "            else:\n",
        "                h_dict[dst_type] = h\n",
        "\n",
        "        # Apply fully connected layers to each node type\n",
        "        for ntype in h_dict:\n",
        "            h_dict[ntype] = self.fc[ntype](h_dict[ntype])\n",
        "\n",
        "        return h_dict\n",
        "\n",
        "# Initialize model parameters using Xavier initialization\n",
        "def init_weights(m):\n",
        "    if isinstance(m, nn.Linear):\n",
        "        nn.init.xavier_uniform_(m.weight)\n",
        "        if m.bias is not None:\n",
        "            nn.init.zeros_(m.bias)\n",
        "    elif isinstance(m, CustomGATConv):\n",
        "        for param in m.parameters():\n",
        "            if param.dim() > 1:\n",
        "                nn.init.xavier_uniform_(param)\n",
        "\n",
        "def initialize_gat_models(hidden_dim, output_dim, num_heads, layer_structure, dropout, subgraphs):\n",
        "    gat_models = [\n",
        "        MetaPathGAT(hidden_dim, output_dim, num_heads=num_heads,\n",
        "                    etypes=sg.canonical_etypes,\n",
        "                    ntypes=sg.ntypes,\n",
        "                    layer_structure=layer_structure,\n",
        "                    dropout=dropout)\n",
        "        for sg in subgraphs\n",
        "    ]\n",
        "\n",
        "    for gat_model in gat_models:\n",
        "        gat_model.apply(init_weights)\n",
        "\n",
        "    print(\"Initialized GAT models with Xavier initialization.\")\n",
        "    return gat_models\n",
        "\n",
        "# MLP model for BCEWithLogitsLoss compliance\n",
        "class MLPModel(nn.Module):\n",
        "    def __init__(self, input_dim, hidden_dim, dropout):\n",
        "        super(MLPModel, self).__init__()\n",
        "        self.mlp = nn.Sequential(\n",
        "            nn.Linear(input_dim, hidden_dim),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(dropout),\n",
        "            nn.Linear(hidden_dim, hidden_dim),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(dropout),\n",
        "            nn.Linear(hidden_dim, hidden_dim),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(dropout),\n",
        "            nn.Linear(hidden_dim, hidden_dim // 2),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(dropout),\n",
        "            nn.Linear(hidden_dim // 2, 1)\n",
        "        )\n",
        "\n",
        "    def forward(self, user_emb, book_emb):\n",
        "        x = torch.cat([user_emb, book_emb], dim=1)\n",
        "        return self.mlp(x)\n",
        "\n",
        "# Training and verification process\n",
        "def train_and_validate_model(train_meta_path_subgraphs, val_meta_path_subgraphs, model_params):\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "    # Preparing models\n",
        "    gat_model_params = {k: v for k, v in model_params.items() if k != 'lr'}\n",
        "    train_gat_models = initialize_gat_models(**gat_model_params, subgraphs=train_meta_path_subgraphs)\n",
        "    val_gat_models = initialize_gat_models(**gat_model_params, subgraphs=val_meta_path_subgraphs)\n",
        "\n",
        "\n",
        "    input_dim = 128\n",
        "    model = MLPModel(input_dim, 512, model_params['dropout']).to(device)\n",
        "\n",
        "    params = list(model.parameters())\n",
        "    for gat_model in train_gat_models:\n",
        "        gat_model.to(device)\n",
        "        params += list(gat_model.parameters())\n",
        "\n",
        "    optimizer = optim.Adam(params, lr=model_params['lr'])\n",
        "\n",
        "\n",
        "    scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=50)\n",
        "\n",
        "    # use BCEWithLogitsLoss\n",
        "    criterion = nn.BCEWithLogitsLoss()\n",
        "\n",
        "    best_loss = float('inf')\n",
        "    patience_counter = 0\n",
        "    early_stopping_patience = 3\n",
        "    best_model = None\n",
        "\n",
        "    # batch_size\n",
        "    batch_size = 2048\n",
        "\n",
        "    max_epochs = 50\n",
        "    for epoch in range(max_epochs):\n",
        "        # Putting models into training mode\n",
        "        model.train()\n",
        "        for gat_model in train_gat_models:\n",
        "            gat_model.train()\n",
        "\n",
        "        total_loss = 0.0\n",
        "        total_samples = 0\n",
        "\n",
        "        # ===== Training session =====\n",
        "        for i, (gat_model, subgraph) in enumerate(zip(train_gat_models, train_meta_path_subgraphs)):\n",
        "            # Make sure there is an edge ('user','rated','book') in the drawing.\n",
        "            if ('user', 'rated', 'book') in subgraph.canonical_etypes:\n",
        "                user_indices, book_indices = subgraph.edges(etype=('user','rated','book'))\n",
        "                ratings = subgraph.edges['rated'].data['rating'].squeeze().float().to(device)\n",
        "\n",
        "\n",
        "                max_rating = ratings.max()\n",
        "                min_rating = ratings.min()\n",
        "                if max_rating > min_rating:\n",
        "                    ratings = (ratings - min_rating) / (max_rating - min_rating)\n",
        "                else:\n",
        "                    ratings = ratings * 0.0\n",
        "\n",
        "                num_samples = len(ratings)\n",
        "                perm = torch.randperm(num_samples, device=device)\n",
        "\n",
        "                for batch_start in range(0, num_samples, batch_size):\n",
        "                    batch_indices = perm[batch_start:batch_start+batch_size]\n",
        "\n",
        "                    batch_user_indices = user_indices[batch_indices]\n",
        "                    batch_book_indices = book_indices[batch_indices]\n",
        "                    ratings_batch = ratings[batch_indices]\n",
        "\n",
        "\n",
        "                    edge_subgraph = dgl.edge_subgraph(\n",
        "                        subgraph,\n",
        "                        {('user','rated','book'): batch_indices.cpu()},  # It is recommended to move batch_indices to the CPU to ensure dgl compatibility.\n",
        "                        relabel_nodes=False\n",
        "                    )\n",
        "\n",
        "                    #(features)\n",
        "                    features = {}\n",
        "                    for ntype in subgraph.ntypes:\n",
        "                        if 'feat' in subgraph.nodes[ntype].data:\n",
        "                            node_ids = edge_subgraph.nodes(ntype)\n",
        "                            features[ntype] = subgraph.nodes[ntype].data['feat'][node_ids].to(device)\n",
        "\n",
        "                    # Calculate the output from the GAT\n",
        "                    h_dict = gat_model(edge_subgraph.to(device), features)\n",
        "\n",
        "                    user_emb = h_dict.get('user', None)\n",
        "                    book_emb = h_dict.get('book', None)\n",
        "\n",
        "\n",
        "                    if user_emb is not None:\n",
        "                        user_emb = nn.functional.normalize(user_emb, p=2, dim=1)\n",
        "                    if book_emb is not None:\n",
        "                        book_emb = nn.functional.normalize(book_emb, p=2, dim=1)\n",
        "\n",
        "\n",
        "                    user_nid_to_idx = {nid.item(): idx for idx, nid in enumerate(edge_subgraph.nodes('user'))}\n",
        "                    book_nid_to_idx = {nid.item(): idx for idx, nid in enumerate(edge_subgraph.nodes('book'))}\n",
        "\n",
        "                    user_emb_indices = torch.tensor(\n",
        "                        [user_nid_to_idx[nid.item()] for nid in batch_user_indices],\n",
        "                        dtype=torch.long, device=device\n",
        "                    )\n",
        "                    book_emb_indices = torch.tensor(\n",
        "                        [book_nid_to_idx[nid.item()] for nid in batch_book_indices],\n",
        "                        dtype=torch.long, device=device\n",
        "                    )\n",
        "\n",
        "                    user_emb_batch = user_emb[user_emb_indices]\n",
        "                    book_emb_batch = book_emb[book_emb_indices]\n",
        "\n",
        "                    output = model(user_emb_batch, book_emb_batch).squeeze()\n",
        "\n",
        "\n",
        "                    loss = criterion(output, ratings_batch)\n",
        "\n",
        "                    optimizer.zero_grad()\n",
        "                    loss.backward()\n",
        "\n",
        "\n",
        "                    torch.nn.utils.clip_grad_norm_(params, max_norm=1.0)\n",
        "\n",
        "                    optimizer.step()\n",
        "\n",
        "                    total_loss += loss.item() * len(ratings_batch)\n",
        "                    total_samples += len(ratings_batch)\n",
        "\n",
        "        avg_train_loss = total_loss / total_samples if total_samples > 0 else 0.0\n",
        "        print(f\"[Epoch {epoch+1:02d}] Training Loss: {avg_train_loss:.6f}\")\n",
        "\n",
        "        # ===== (Validation) =====\n",
        "        model.eval()\n",
        "        for gat_model in val_gat_models:\n",
        "            gat_model.eval()\n",
        "\n",
        "        val_loss = 0.0\n",
        "        val_samples = 0\n",
        "\n",
        "        with torch.no_grad():\n",
        "            for i, (gat_model, subgraph) in enumerate(zip(val_gat_models, val_meta_path_subgraphs)):\n",
        "                if ('user', 'rated', 'book') in subgraph.canonical_etypes:\n",
        "                    user_indices, book_indices = subgraph.edges(etype=('user','rated','book'))\n",
        "                    ratings = subgraph.edges['rated'].data['rating'].squeeze().float().to(device)\n",
        "\n",
        "                    max_rating = ratings.max()\n",
        "                    min_rating = ratings.min()\n",
        "                    if max_rating > min_rating:\n",
        "                        ratings = (ratings - min_rating) / (max_rating - min_rating)\n",
        "                    else:\n",
        "                        ratings = ratings * 0.0\n",
        "\n",
        "                    num_samples = len(ratings)\n",
        "\n",
        "                    for batch_start in range(0, num_samples, batch_size):\n",
        "                        batch_indices = torch.arange(batch_start, min(batch_start+batch_size, num_samples), device=device)\n",
        "\n",
        "                        batch_user_indices = user_indices[batch_indices]\n",
        "                        batch_book_indices = book_indices[batch_indices]\n",
        "                        ratings_batch = ratings[batch_indices]\n",
        "\n",
        "                        edge_subgraph = dgl.edge_subgraph(\n",
        "                            subgraph,\n",
        "                            {('user', 'rated', 'book'): batch_indices.cpu()},\n",
        "                            relabel_nodes=False\n",
        "                        )\n",
        "\n",
        "                        features = {}\n",
        "                        for ntype in subgraph.ntypes:\n",
        "                            if 'feat' in subgraph.nodes[ntype].data:\n",
        "                                node_ids = edge_subgraph.nodes(ntype)\n",
        "                                features[ntype] = subgraph.nodes[ntype].data['feat'][node_ids].to(device)\n",
        "\n",
        "                        h_dict = gat_model(edge_subgraph.to(device), features)\n",
        "\n",
        "                        user_emb = h_dict.get('user', None)\n",
        "                        book_emb = h_dict.get('book', None)\n",
        "\n",
        "                        if user_emb is not None:\n",
        "                            user_emb = nn.functional.normalize(user_emb, p=2, dim=1)\n",
        "                        if book_emb is not None:\n",
        "                            book_emb = nn.functional.normalize(book_emb, p=2, dim=1)\n",
        "\n",
        "                        user_nid_to_idx = {nid.item(): idx for idx, nid in enumerate(edge_subgraph.nodes('user'))}\n",
        "                        book_nid_to_idx = {nid.item(): idx for idx, nid in enumerate(edge_subgraph.nodes('book'))}\n",
        "\n",
        "                        user_emb_indices = torch.tensor(\n",
        "                            [user_nid_to_idx[nid.item()] for nid in batch_user_indices],\n",
        "                            dtype=torch.long, device=device\n",
        "                        )\n",
        "                        book_emb_indices = torch.tensor(\n",
        "                            [book_nid_to_idx[nid.item()] for nid in batch_book_indices],\n",
        "                            dtype=torch.long, device=device\n",
        "                        )\n",
        "\n",
        "                        user_emb_batch = user_emb[user_emb_indices]\n",
        "                        book_emb_batch = book_emb[book_emb_indices]\n",
        "\n",
        "                        output = model(user_emb_batch, book_emb_batch).squeeze()\n",
        "                        loss = criterion(output, ratings_batch)\n",
        "\n",
        "                        val_loss += loss.item() * len(ratings_batch)\n",
        "                        val_samples += len(ratings_batch)\n",
        "\n",
        "        avg_val_loss = val_loss / val_samples if val_samples > 0 else 0.0\n",
        "        print(f\"[Epoch {epoch+1:02d}] Validation Loss: {avg_val_loss:.6f}\")\n",
        "\n",
        "        # Check out the best results for early cessation.\n",
        "        if avg_val_loss < best_loss:\n",
        "            best_loss = avg_val_loss\n",
        "            best_model = model.state_dict()\n",
        "            patience_counter = 0\n",
        "        else:\n",
        "            patience_counter += 1\n",
        "\n",
        "        if patience_counter >= early_stopping_patience:\n",
        "            print(\"Early stopping triggered!\")\n",
        "            break\n",
        "\n",
        "        scheduler.step()\n",
        "\n",
        "    # Save the best MLP model\n",
        "    if best_model is not None:\n",
        "        torch.save(best_model, '/content/drive/MyDrive/graph/metapath_subgraphs/mean_model.pth')\n",
        "        print(f\"Best Model Validation Loss: {best_loss:.6f}\")\n",
        "\n",
        "    # Save GAT model weights\n",
        "    for i, gat_model in enumerate(train_gat_models):\n",
        "        torch.save(gat_model.state_dict(), f'/content/drive/MyDrive/graph/metapath_subgraphs/mean_gat_{i}.pth')\n",
        "\n",
        "# ========== Initialize parameters and call the function==========\n",
        "\n",
        "model_params = {\n",
        "    'hidden_dim': 64,\n",
        "    'output_dim': 64,\n",
        "    'num_heads': 16,\n",
        "    'layer_structure': [[64, 32, 16], [64, 32, 16], [64], [64]],\n",
        "    'dropout': 0.2,\n",
        "    'lr': 1e-3\n",
        "}\n",
        "\n",
        "train_and_validate_model(train_meta_path_subgraphs, val_meta_path_subgraphs, model_params)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Fs2dZjaeh93m",
        "outputId": "178e4e2b-8c2b-4209-87d9-abce983da727"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loaded subgraph 1 from /content/drive/MyDrive/graph/metapath_subgraphs/test_subgraph_genre_path.bin\n",
            "Loaded subgraph 2 from /content/drive/MyDrive/graph/metapath_subgraphs/test_subgraph_author_path.bin\n",
            "Loaded subgraph 3 from /content/drive/MyDrive/graph/metapath_subgraphs/test_subgraph_similar_path.bin\n",
            "Initialized GAT models with Xavier initialization.\n",
            "Loaded GAT model 0 for evaluation.\n",
            "Loaded GAT model 1 for evaluation.\n",
            "Loaded GAT model 2 for evaluation.\n",
            "Loaded MLP model for evaluation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-11-409a06ccc0cb>:221: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  user_emb = torch.tensor(user_emb_dict[uid], dtype=torch.float32).to(device).unsqueeze(0)\n",
            "<ipython-input-11-409a06ccc0cb>:222: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  book_emb = torch.tensor(book_emb_dict[bid], dtype=torch.float32).to(device).unsqueeze(0)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test MSE: 0.2497\n",
            "Test RMSE: 0.4997\n",
            "Test MAE: 0.4992\n",
            "Test Accuracy: 0.5143\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-11-409a06ccc0cb>:263: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  user_emb = torch.tensor(user_emb_dict[uid], dtype=torch.float32).to(device).unsqueeze(0)\n",
            "<ipython-input-11-409a06ccc0cb>:264: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  book_emb = torch.tensor(book_emb_dict[bid], dtype=torch.float32).to(device).unsqueeze(0)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Test Precision@5: 0.3056\n",
            "Test Recall@5: 0.8123\n",
            "\n",
            "Test Precision@10: 0.2061\n",
            "Test Recall@10: 0.9120\n",
            "\n",
            "Test Precision@15: 0.1491\n",
            "Test Recall@15: 0.9338\n",
            "\n",
            "Test Precision@20: 0.1244\n",
            "Test Recall@20: 0.9592\n",
            "\n",
            "Test Precision@25: 0.1064\n",
            "Test Recall@25: 0.9723\n"
          ]
        }
      ],
      "source": [
        "# -*- coding: utf-8 -*-\n",
        "\"\"\"\n",
        "Evaluation of GAT-MLP Models Using mean-based Attention Aggregation\n",
        "==============================================================\n",
        "Summary\n",
        "-------\n",
        "This script evaluates trained Graph Attention Network (GAT) and Multi-Layer Perceptron (MLP) models on test datasets:\n",
        "\n",
        "1. Loads previously trained GAT and MLP models from disk.\n",
        "2. Computes user and book embeddings using GAT models for different semantic metapaths (author-based, genre-based, similarity-based).\n",
        "3. Predicts ratings using the MLP model with these embeddings.\n",
        "4. Calculates evaluation metrics (MSE, RMSE, MAE, Accuracy) and ranking metrics (Precision@K, Recall@K) for multiple values of K.\n",
        "\"\"\"\n",
        "import torch\n",
        "import dgl\n",
        "import torch.nn as nn\n",
        "import numpy as np\n",
        "from sklearn.metrics import mean_squared_error, mean_absolute_error, accuracy_score\n",
        "from collections import defaultdict\n",
        "import torch.optim as optim\n",
        "import dgl.nn as dglnn\n",
        "\n",
        "# load_subgraphs\n",
        "def load_subgraphs(paths):\n",
        "    subgraphs = [dgl.load_graphs(path)[0][0] for path in paths]\n",
        "    for i, subgraph in enumerate(subgraphs):\n",
        "        print(f\"Loaded subgraph {i+1} from {paths[i]}\")\n",
        "    return subgraphs\n",
        "\n",
        "\n",
        "test_subgraph_paths = [\n",
        "    '/content/drive/MyDrive/graph/metapath_subgraphs/test_subgraph_genre_path.bin',\n",
        "    '/content/drive/MyDrive/graph/metapath_subgraphs/test_subgraph_author_path.bin',\n",
        "    '/content/drive/MyDrive/graph/metapath_subgraphs/test_subgraph_similar_path.bin'\n",
        "]\n",
        "\n",
        "test_meta_path_subgraphs = load_subgraphs(test_subgraph_paths)\n",
        "\n",
        "# Define a custom layer for GATConv to store attention weights\n",
        "class CustomGATConv(dglnn.GATConv):\n",
        "    def forward(self, graph, feat):\n",
        "        # Perform forward pass of GATConv and get attention weights\n",
        "        h, attn_weights = super().forward(graph, feat, get_attention=True)\n",
        "        # Keep attention weights in the same layer\n",
        "        self.attention_weights = attn_weights\n",
        "        return h\n",
        "\n",
        "# Defining a GAT model using `CustomGATConv`\n",
        "class MetaPathGAT(nn.Module):\n",
        "    def __init__(self, hidden_dim, out_dim, num_heads, etypes, ntypes, layer_structure, dropout):\n",
        "        super(MetaPathGAT, self).__init__()\n",
        "        self.hidden_dim = hidden_dim\n",
        "        self.num_heads = num_heads\n",
        "        self.etypes = etypes\n",
        "        self.layer_structure = layer_structure\n",
        "\n",
        "        # Define GAT layers for each edge type using `CustomGATConv`\n",
        "        self.gat_layers = nn.ModuleDict({\n",
        "            str(etype): nn.ModuleList([\n",
        "                CustomGATConv(\n",
        "                    hidden_dim, hidden_dim // num_heads, num_heads=num_heads,\n",
        "                    feat_drop=dropout, attn_drop=dropout, activation=nn.ELU(), allow_zero_in_degree=True\n",
        "                )\n",
        "                for _ in range(len(layer_structure))\n",
        "            ]) for etype in etypes\n",
        "        })\n",
        "\n",
        "        # Fully connected layers for each node type\n",
        "        self.fc = nn.ModuleDict({\n",
        "            ntype: nn.Linear(hidden_dim // num_heads, out_dim) for ntype in ntypes\n",
        "        })\n",
        "\n",
        "\n",
        "        self.attention_weights = defaultdict(list)\n",
        "\n",
        "    def forward(self, g, features):\n",
        "        h_dict = {}\n",
        "        self.attention_weights.clear()\n",
        "\n",
        "        for etype in g.canonical_etypes:\n",
        "            src_type, edge_type, dst_type = etype\n",
        "            etype_key = str(etype)\n",
        "\n",
        "            if etype_key not in self.gat_layers:\n",
        "                continue\n",
        "\n",
        "            if src_type not in features or dst_type not in features:\n",
        "                continue\n",
        "\n",
        "            h_src = features[src_type]\n",
        "            h_dst = features[dst_type] if src_type != dst_type else h_src\n",
        "\n",
        "            # Apply GAT layers and save attention weights\n",
        "            for layer in self.gat_layers[etype_key]:\n",
        "                h = layer(g[etype], (h_src, h_dst))\n",
        "\n",
        "                # save attention weights\n",
        "                self.attention_weights[etype_key].append(layer.attention_weights)\n",
        "\n",
        "            h = h.mean(dim=1)\n",
        "\n",
        "            if dst_type in h_dict:\n",
        "                h_dict[dst_type] += h\n",
        "            else:\n",
        "                h_dict[dst_type] = h\n",
        "\n",
        "        for ntype in h_dict:\n",
        "            h_dict[ntype] = self.fc[ntype](h_dict[ntype])\n",
        "\n",
        "        return h_dict\n",
        "\n",
        "# Initialize model parameters using Xavier initialization\n",
        "def init_weights(m):\n",
        "    if isinstance(m, nn.Linear):\n",
        "        nn.init.xavier_uniform_(m.weight)\n",
        "    elif isinstance(m, CustomGATConv):\n",
        "        for param in m.parameters():\n",
        "            if param.dim() > 1:\n",
        "                nn.init.xavier_uniform_(param)\n",
        "\n",
        "def initialize_gat_models(hidden_dim, output_dim, num_heads, layer_structure, dropout, subgraphs):\n",
        "    gat_models = [\n",
        "        MetaPathGAT(hidden_dim, output_dim, num_heads=num_heads, etypes=sg.canonical_etypes, ntypes=sg.ntypes,\n",
        "                    layer_structure=layer_structure, dropout=dropout)\n",
        "        for sg in subgraphs\n",
        "    ]\n",
        "    for gat_model in gat_models:\n",
        "        gat_model.apply(init_weights)\n",
        "    print(\"Initialized GAT models with Xavier initialization.\")\n",
        "    return gat_models\n",
        "\n",
        "# MLP model\n",
        "class MLPModel(nn.Module):\n",
        "    def __init__(self, input_dim, hidden_dim, dropout):\n",
        "        super(MLPModel, self).__init__()\n",
        "        self.mlp = nn.Sequential(\n",
        "            nn.Linear(input_dim, hidden_dim),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(dropout),\n",
        "            nn.Linear(hidden_dim, hidden_dim),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(dropout),\n",
        "            nn.Linear(hidden_dim, hidden_dim),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(dropout),\n",
        "            nn.Linear(hidden_dim, hidden_dim // 2),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(dropout),\n",
        "            nn.Linear(hidden_dim // 2, 1)\n",
        "\n",
        "        )\n",
        "\n",
        "    def forward(self, user_emb, book_emb):\n",
        "        x = torch.cat([user_emb, book_emb], dim=1)\n",
        "        return self.mlp(x)\n",
        "\n",
        "# Extract and unify embeddings\n",
        "def get_embeddings_and_concatenate(gat_models, subgraphs, device):\n",
        "    user_emb_dict = {}\n",
        "    book_emb_dict = {}\n",
        "    for gat_model, subgraph in zip(gat_models, subgraphs):\n",
        "        features = {}\n",
        "        for ntype in subgraph.ntypes:\n",
        "            if 'feat' in subgraph.nodes[ntype].data:\n",
        "                node_ids = subgraph.nodes(ntype)\n",
        "                features[ntype] = subgraph.nodes[ntype].data['feat'][node_ids].to(device)\n",
        "        h_dict = gat_model(subgraph.to(device), features)\n",
        "        for ntype in ['user', 'book']:\n",
        "            if ntype in h_dict:\n",
        "                embeddings = h_dict[ntype].detach().cpu()\n",
        "                node_ids = subgraph.nodes(ntype).detach().cpu().numpy()\n",
        "                for nid, emb in zip(node_ids, embeddings):\n",
        "                    if ntype == 'user':\n",
        "                        user_emb_dict[nid] = emb\n",
        "                    else:\n",
        "                        book_emb_dict[nid] = emb\n",
        "    return user_emb_dict, book_emb_dict\n",
        "\n",
        "# evaluate_model\n",
        "def evaluate_model(test_meta_path_subgraphs, model_params):\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "    # Update model parameters to match training.\n",
        "    gat_model_params = {k: v for k, v in model_params.items() if k != 'lr'}\n",
        "    test_gat_models = initialize_gat_models(**gat_model_params, subgraphs=test_meta_path_subgraphs)\n",
        "\n",
        "    # load GAT Weights\n",
        "    for i, gat_model in enumerate(test_gat_models):\n",
        "        gat_model.load_state_dict(torch.load(f'/content/drive/MyDrive/graph/metapath_subgraphs/mean_gat_{i}.pth', map_location=device))\n",
        "        gat_model.to(device)\n",
        "        gat_model.eval()\n",
        "        print(f\"Loaded GAT model {i} for evaluation.\")\n",
        "\n",
        "    input_dim = 128\n",
        "    model = MLPModel(input_dim, 512, model_params['dropout'])\n",
        "    model.load_state_dict(torch.load('/content/drive/MyDrive/graph/metapath_subgraphs/mean_model.pth', map_location=device))\n",
        "    model = model.to(device)\n",
        "    model.eval()\n",
        "    print(\"Loaded MLP model for evaluation.\")\n",
        "\n",
        "    # Extract embeddings\n",
        "    user_emb_dict, book_emb_dict = get_embeddings_and_concatenate(test_gat_models, test_meta_path_subgraphs, device)\n",
        "\n",
        "    # Check for embeddings\n",
        "    if user_emb_dict and book_emb_dict:\n",
        "        total_y_true = []\n",
        "        total_y_pred = []\n",
        "\n",
        "        # Iterate through all subgraphs of the test.\n",
        "        for subgraph in test_meta_path_subgraphs:\n",
        "            if ('user', 'rated', 'book') in subgraph.canonical_etypes:\n",
        "                rated_edges = subgraph.edges(etype=('user', 'rated', 'book'))\n",
        "                user_indices, book_indices = rated_edges[0], rated_edges[1]\n",
        "\n",
        "                ratings = subgraph.edges['rated'].data['rating'].squeeze().float()\n",
        "                ratings = ratings.to(device)\n",
        "\n",
        "\n",
        "                max_rating = ratings.max()\n",
        "                min_rating = ratings.min()\n",
        "                ratings = (ratings - min_rating) / (max_rating - min_rating)\n",
        "\n",
        "\n",
        "                ratings_binary = (ratings > 0.5).float()\n",
        "\n",
        "                user_ids = user_indices.detach().cpu().numpy()\n",
        "                book_ids = book_indices.detach().cpu().numpy()\n",
        "                ratings_np = ratings_binary.detach().cpu().numpy()\n",
        "\n",
        "                # Store actual ratings and predicate\n",
        "                for uid, bid, rating in zip(user_ids, book_ids, ratings_np):\n",
        "                    if uid in user_emb_dict and bid in book_emb_dict:\n",
        "                        user_emb = torch.tensor(user_emb_dict[uid], dtype=torch.float32).to(device).unsqueeze(0)\n",
        "                        book_emb = torch.tensor(book_emb_dict[bid], dtype=torch.float32).to(device).unsqueeze(0)\n",
        "\n",
        "                        with torch.no_grad():\n",
        "                            output = model(user_emb, book_emb).squeeze().cpu().numpy()\n",
        "                            total_y_true.append(rating)\n",
        "\n",
        "                            output_sigmoid = 1 / (1 + np.exp(-output))\n",
        "                            total_y_pred.append(output_sigmoid)\n",
        "\n",
        "\n",
        "        y_true = np.array(total_y_true)\n",
        "        y_pred = np.array(total_y_pred)\n",
        "\n",
        "       #  calculation metrics\n",
        "        mse = mean_squared_error(y_true, y_pred)\n",
        "        rmse = np.sqrt(mse)\n",
        "        mae = mean_absolute_error(y_true, y_pred)\n",
        "\n",
        "        # Accuracy calculation based on a threshold of 0.5\n",
        "        y_pred_binary = (y_pred > 0.5).astype(int)\n",
        "        accuracy = accuracy_score(y_true, y_pred_binary)\n",
        "\n",
        "        print(f\"Test MSE: {mse:.4f}\")\n",
        "        print(f\"Test RMSE: {rmse:.4f}\")\n",
        "        print(f\"Test MAE: {mae:.4f}\")\n",
        "        print(f\"Test Accuracy: {accuracy:.4f}\")\n",
        "\n",
        "        # Compute Precision@K and Recall@K for multiple values ​​of K\n",
        "        user_pred_scores = defaultdict(list)\n",
        "        for subgraph in test_meta_path_subgraphs:\n",
        "            if ('user', 'rated', 'book') in subgraph.canonical_etypes:\n",
        "                rated_edges = subgraph.edges(etype=('user', 'rated', 'book'))\n",
        "                user_indices, book_indices = rated_edges[0], rated_edges[1]\n",
        "                ratings = subgraph.edges['rated'].data['rating'].squeeze().float()\n",
        "                ratings = (ratings - ratings.min()) / (ratings.max() - ratings.min())\n",
        "\n",
        "                user_ids = user_indices.detach().cpu().numpy()\n",
        "                book_ids = book_indices.detach().cpu().numpy()\n",
        "\n",
        "                for uid, bid in zip(user_ids, book_ids):\n",
        "                    if uid in user_emb_dict and bid in book_emb_dict:\n",
        "                        user_emb = torch.tensor(user_emb_dict[uid], dtype=torch.float32).to(device).unsqueeze(0)\n",
        "                        book_emb = torch.tensor(book_emb_dict[bid], dtype=torch.float32).to(device).unsqueeze(0)\n",
        "\n",
        "                        with torch.no_grad():\n",
        "                            output = model(user_emb, book_emb).squeeze().cpu().numpy()\n",
        "                            output_sigmoid = 1 / (1 + np.exp(-output))\n",
        "                            user_pred_scores[uid].append((output_sigmoid, bid))\n",
        "\n",
        "        ks = [5, 10, 15, 20, 25]\n",
        "        for k in ks:\n",
        "            precisions, recalls = [], []\n",
        "            for uid, scores in user_pred_scores.items():\n",
        "                # Sort books based on predictive scores\n",
        "                scores_sorted = sorted(scores, key=lambda x: x[0], reverse=True)\n",
        "                recommended_books = [bid for score, bid in scores_sorted[:k]]\n",
        "                #Get real books that users have rated.\n",
        "                true_books = subgraph.out_edges(uid, etype='rated')[1].detach().cpu().numpy()\n",
        "                hit_set = set(recommended_books) & set(true_books)\n",
        "                precision = len(hit_set) / k\n",
        "                recall = len(hit_set) / len(true_books) if len(true_books) > 0 else 0\n",
        "                precisions.append(precision)\n",
        "                recalls.append(recall)\n",
        "            avg_precision = np.mean(precisions) if precisions else 0\n",
        "            avg_recall = np.mean(recalls) if recalls else 0\n",
        "            print(f\"\\nTest Precision@{k}: {avg_precision:.4f}\")\n",
        "            print(f\"Test Recall@{k}: {avg_recall:.4f}\")\n",
        "\n",
        "    else:\n",
        "        print(\"Error: Could not obtain embeddings for users and books.\")\n",
        "\n",
        "# # Setting parameters and evaluating the model\n",
        "model_params = {\n",
        "    'hidden_dim': 64,\n",
        "    'output_dim': 64,\n",
        "    'num_heads': 16,\n",
        "    'layer_structure': [[64, 32, 16], [64, 32, 16], [64], [64]],\n",
        "    'dropout': 0.2,\n",
        "    'lr': 1e-1\n",
        "}\n",
        "\n",
        "evaluate_model(test_meta_path_subgraphs, model_params)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-ELrq4MMttTj",
        "outputId": "71aaa174-3432-4577-ae9b-19e55c6b81fe"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Initialized GAT models with Xavier initialization.\n",
            "\n",
            "========== EXAMPLE: Single path found ==========\n",
            "User: 11241844, Book: 22585423\n",
            "Found path in 1 Subgraph(s)\n",
            "  Subgraph 1: normalized importance = 1.0000\n",
            "    user-rated-book: 0.0685\n",
            "    book-categorized_as-genre: 0.0000\n",
            "    genre-categorized_in-book: 0.3883\n",
            "\n",
            "========== EXAMPLE: Double paths found ==========\n",
            "User: 17950029, Book: 3651522\n",
            "Found path in 2 Subgraphs\n",
            "  Subgraph 1: normalized importance = 0.3030\n",
            "    user-rated-book: 0.2576\n",
            "    book-categorized_as-genre: 0.0000\n",
            "    genre-categorized_in-book: 0.3794\n",
            "  Subgraph 2: normalized importance = 0.6970\n",
            "    user-rated-book: 0.2653\n",
            "    book-written_by-author: 0.2000\n",
            "    author-wrote-book: 1.0000\n",
            "\n",
            "========== EXAMPLE: Triple paths found ==========\n",
            "User: 4635289, Book: 3153116\n",
            "Found path in 3 Subgraphs (All three!)\n",
            "  Subgraph 1: normalized importance = 0.2089\n",
            "    user-rated-book: 0.0210\n",
            "    book-categorized_as-genre: 0.0000\n",
            "    genre-categorized_in-book: 0.3935\n",
            "  Subgraph 2: normalized importance = 0.7668\n",
            "    user-rated-book: 0.0215\n",
            "    book-written_by-author: 0.5000\n",
            "    author-wrote-book: 1.0000\n",
            "  Subgraph 3: normalized importance = 0.0244\n",
            "    user-similar_to-user: 0.0185\n",
            "    user-rated-book: 0.0298\n"
          ]
        }
      ],
      "source": [
        "# -*- coding: utf-8 -*-\n",
        "\"\"\"\n",
        "Explainability Case Study with Normalized Importance Scores\n",
        "===========================================================\n",
        "Summary\n",
        "-------\n",
        "Key functions:\n",
        "1. `explain_recommendation`: Traces a user-to-book metapath within a given subgraph and extracts average attention weights to quantify the path’s importance.\n",
        "2. `evaluate_model`: Iteratively samples user-book pairs and evaluates their explainability across up to three subgraphs, each representing a different semantic metapath (e.g., user-genre-book, user-author-book, similar-users).\n",
        "3. The script collects attention scores for each path, normalizes the total importance across all paths, and prints detailed breakdowns for:\n",
        "   - One-path explanations (Single Subgraph),\n",
        "   - Two-path explanations (Double Subgraphs),\n",
        "   - Three-path explanations (All Subgraphs).\n",
        "4. Outputs include:\n",
        "   - The normalized importance of each path.\n",
        "   - The individual edge types and their associated attention values within each metapath.\n",
        "\"\"\"\n",
        "import random\n",
        "\n",
        "def explain_recommendation(subgraph, user_id, book_id, gat_model, metapath):\n",
        "    device = next(gat_model.parameters()).device\n",
        "\n",
        "    # Create maps between orginal node IDs and node IDs in the subgraph\n",
        "    node_id_maps = {}\n",
        "    for ntype in subgraph.ntypes:\n",
        "        if 'original_id' in subgraph.nodes[ntype].data:\n",
        "            orig_ids = subgraph.nodes[ntype].data['original_id']\n",
        "        else:\n",
        "            orig_ids = subgraph.nodes[ntype].data['nid']\n",
        "        node_id_maps[ntype] = {nid.item(): idx for idx, nid in enumerate(orig_ids)}\n",
        "\n",
        "    # Convert original contract IDs to internal IDs\n",
        "    user_idx = node_id_maps['user'].get(user_id)\n",
        "    book_idx = node_id_maps['book'].get(book_id)\n",
        "\n",
        "    if user_idx is None or book_idx is None:\n",
        "        return None\n",
        "\n",
        "    # Track the path\n",
        "    paths = []\n",
        "    current_nodes = [user_idx]\n",
        "\n",
        "    for (src_type, e_type, dst_type) in metapath:\n",
        "        next_nodes = []\n",
        "        attention_weights = []\n",
        "\n",
        "        for node in current_nodes:\n",
        "            successors = subgraph.successors(node, etype=(src_type, e_type, dst_type))\n",
        "            if len(successors) == 0:\n",
        "                continue\n",
        "            next_nodes.extend(successors.tolist())\n",
        "\n",
        "            attention_layer = gat_model.gat_layers[str((src_type, e_type, dst_type))][0]\n",
        "            attn_weights = attention_layer.attention_weights\n",
        "            edge_idx = subgraph.edge_ids(node, successors, etype=(src_type, e_type, dst_type))\n",
        "\n",
        "            weights = attn_weights[edge_idx].mean().item()\n",
        "            attention_weights.append(weights)\n",
        "\n",
        "        if len(next_nodes) == 0:\n",
        "            return None\n",
        "\n",
        "        current_nodes = list(set(next_nodes))\n",
        "        paths.append((f\"{src_type}-{e_type}-{dst_type}\", attention_weights))\n",
        "\n",
        "    if book_idx not in current_nodes:\n",
        "        return None\n",
        "\n",
        "    total_importance = 0\n",
        "    importance_details = []\n",
        "    for etype, weights in paths:\n",
        "        avg_weight = np.mean(weights)\n",
        "        total_importance += avg_weight\n",
        "        importance_details.append((etype, avg_weight))\n",
        "\n",
        "    return total_importance, importance_details\n",
        "\n",
        "\n",
        "def evaluate_model(test_meta_path_subgraphs, model_params):\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "    gat_model_params = {k: v for k, v in model_params.items() if k != 'lr'}\n",
        "    test_gat_models = initialize_gat_models(**gat_model_params, subgraphs=test_meta_path_subgraphs)\n",
        "\n",
        "    # تحميل أوزان GAT\n",
        "    for i, gat_model in enumerate(test_gat_models):\n",
        "        gat_model.load_state_dict(torch.load(\n",
        "            f'/content/drive/MyDrive/graph/metapath_subgraphs/mean_gat_{i}.pth',\n",
        "            map_location=device\n",
        "        ))\n",
        "        gat_model.to(device)\n",
        "        gat_model.eval()\n",
        "\n",
        "    # Initialize the MLP model and load the weights.\n",
        "    input_dim = 128\n",
        "    model = MLPModel(input_dim, 512, model_params['dropout'])\n",
        "    model.load_state_dict(torch.load(\n",
        "        '/content/drive/MyDrive/graph/metapath_subgraphs/mean_model.pth',\n",
        "        map_location=device\n",
        "    ))\n",
        "    model.to(device)\n",
        "    model.eval()\n",
        "\n",
        "    # Extract embeddings\n",
        "    user_emb_dict, book_emb_dict = get_embeddings_and_concatenate(test_gat_models, test_meta_path_subgraphs, device)\n",
        "\n",
        "\n",
        "    max_cases = 50\n",
        "    found_cases = 0\n",
        "\n",
        "    # # We will store here if we find examples of path in 1, 2, 3 Subgraphs\n",
        "    found_single_case = False\n",
        "    found_double_case = False\n",
        "    found_triple_case = False\n",
        "\n",
        "    # We search within a loop until we find 3 different examples or we exceed max_cases.\n",
        "    while found_cases < max_cases and not (found_single_case and found_double_case and found_triple_case):\n",
        "        for i, (subgraph, gat_model) in enumerate(zip(test_meta_path_subgraphs, test_gat_models)):\n",
        "            # Ensure the presence of edge_type ('user', 'rated', 'book')\n",
        "            if ('user', 'rated', 'book') not in subgraph.canonical_etypes:\n",
        "                continue\n",
        "\n",
        "            rated_edges = subgraph.edges(etype=('user', 'rated', 'book'))\n",
        "            user_indices, book_indices = rated_edges[0], rated_edges[1]\n",
        "            ratings = subgraph.edges[('user', 'rated', 'book')].data['rating'].squeeze().float().to(device)\n",
        "\n",
        "            # Normalization of ratings\n",
        "            max_rating = ratings.max()\n",
        "            min_rating = ratings.min()\n",
        "            if max_rating > min_rating:\n",
        "                ratings = (ratings - min_rating) / (max_rating - min_rating)\n",
        "\n",
        "            # User IDs and Books\n",
        "            if 'original_id' in subgraph.nodes['user'].data:\n",
        "                user_ids = subgraph.nodes['user'].data['original_id'][user_indices].detach().cpu().numpy()\n",
        "            else:\n",
        "                user_ids = subgraph.nodes['user'].data['nid'][user_indices].detach().cpu().numpy()\n",
        "\n",
        "            if 'original_id' in subgraph.nodes['book'].data:\n",
        "                book_ids = subgraph.nodes['book'].data['original_id'][book_indices].detach().cpu().numpy()\n",
        "            else:\n",
        "                book_ids = subgraph.nodes['book'].data['nid'][book_indices].detach().cpu().numpy()\n",
        "\n",
        "            # Randomly select multiple users\n",
        "            random_users = random.sample(list(user_ids), 5)\n",
        "            for uid in random_users:\n",
        "                # Randomly select a recommended book for the user.\n",
        "                bid = random.choice(book_ids)\n",
        "\n",
        "                results = {}\n",
        "                # Check all Subgraph\n",
        "                for subgraph_index, (sg, gm) in enumerate(zip(test_meta_path_subgraphs, test_gat_models)):\n",
        "                    # Determine the meta path based on subgraph_index\n",
        "                    if subgraph_index == 0:\n",
        "                        metapath = [\n",
        "                            ('user', 'rated', 'book'),\n",
        "                            ('book', 'categorized_as', 'genre'),\n",
        "                            ('genre', 'categorized_in', 'book')\n",
        "                        ]\n",
        "                    elif subgraph_index == 1:\n",
        "                        metapath = [\n",
        "                            ('user', 'rated', 'book'),\n",
        "                            ('book', 'written_by', 'author'),\n",
        "                            ('author', 'wrote', 'book')\n",
        "                        ]\n",
        "                    elif subgraph_index == 2:\n",
        "                        metapath = [\n",
        "                            ('user', 'similar_to', 'user'),\n",
        "                            ('user', 'rated', 'book')\n",
        "                        ]\n",
        "                    else:\n",
        "                        continue\n",
        "\n",
        "                    result = explain_recommendation(sg, uid, bid, gm, metapath)\n",
        "                    if result is not None:\n",
        "                        results[subgraph_index + 1] = result\n",
        "\n",
        "                # Now we know how many paths were discovered.\n",
        "                num_paths_found = len(results)\n",
        "\n",
        "                if num_paths_found == 0:\n",
        "                    continue\n",
        "\n",
        "                # ========= Normalize importance between discovered paths =========\n",
        "\n",
        "                all_importances = [res[0] for res in results.values()]\n",
        "                sum_imp = sum(all_importances)\n",
        "                if sum_imp > 1e-9:\n",
        "                    for k in results:\n",
        "                        old_imp, old_details = results[k]\n",
        "                        new_imp = old_imp / sum_imp\n",
        "                        results[k] = (new_imp, old_details)\n",
        "                # =====================================================\n",
        "\n",
        "                # Print status based on number of tracks\n",
        "                if num_paths_found == 1 and not found_single_case:\n",
        "                    print(\"\\n========== EXAMPLE: Single path found ==========\")\n",
        "                    print(f\"User: {uid}, Book: {bid}\")\n",
        "                    print(f\"Found path in {num_paths_found} Subgraph(s)\")\n",
        "                    for subg_idx, (t_imp, imp_details) in results.items():\n",
        "                        print(f\"  Subgraph {subg_idx}: normalized importance = {t_imp:.4f}\")\n",
        "                        for etype, avg_weight in imp_details:\n",
        "                            print(f\"    {etype}: {avg_weight:.4f}\")\n",
        "                    found_single_case = True\n",
        "                    found_cases += 1\n",
        "\n",
        "                elif num_paths_found == 2 and not found_double_case:\n",
        "                    print(\"\\n========== EXAMPLE: Double paths found ==========\")\n",
        "                    print(f\"User: {uid}, Book: {bid}\")\n",
        "                    print(f\"Found path in {num_paths_found} Subgraphs\")\n",
        "                    for subg_idx, (t_imp, imp_details) in results.items():\n",
        "                        print(f\"  Subgraph {subg_idx}: normalized importance = {t_imp:.4f}\")\n",
        "                        for etype, avg_weight in imp_details:\n",
        "                            print(f\"    {etype}: {avg_weight:.4f}\")\n",
        "                    found_double_case = True\n",
        "                    found_cases += 1\n",
        "\n",
        "                elif num_paths_found == 3 and not found_triple_case:\n",
        "                    print(\"\\n========== EXAMPLE: Triple paths found ==========\")\n",
        "                    print(f\"User: {uid}, Book: {bid}\")\n",
        "                    print(f\"Found path in {num_paths_found} Subgraphs (All three!)\")\n",
        "                    for subg_idx, (t_imp, imp_details) in results.items():\n",
        "                        print(f\"  Subgraph {subg_idx}: normalized importance = {t_imp:.4f}\")\n",
        "                        for etype, avg_weight in imp_details:\n",
        "                            print(f\"    {etype}: {avg_weight:.4f}\")\n",
        "                    found_triple_case = True\n",
        "                    found_cases += 1\n",
        "\n",
        "                # If we find all examples (1,2,3) or exceed max_cases => stop\n",
        "                if (found_single_case and found_double_case and found_triple_case) or (found_cases >= max_cases):\n",
        "                    return\n",
        "\n",
        "\n",
        "# ========== Initialize parameters and call the function==========\n",
        "\n",
        "model_params = {\n",
        "    'hidden_dim': 64,\n",
        "    'output_dim': 64,\n",
        "    'num_heads': 16,\n",
        "    'layer_structure': [[64, 32, 16], [64, 32, 16], [64], [64]],\n",
        "    'dropout': 0.2,\n",
        "    'lr': 1e-3\n",
        "}\n",
        "\n",
        "evaluate_model(test_meta_path_subgraphs, model_params)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "Arabic Book Recommendation Explanation Generator (Template-Based)\n",
        "=================================================================\n",
        "Main Features\n",
        "-------------\n",
        "- Loads recommendation paths from a TSV file.\n",
        "- Extracts metadata (title, author, genre) from a local JSON file.\n",
        "- Determines the recommendation type by analyzing the graph path.\n",
        "- Generates a short explanation in Modern Standard Arabic using static templates.\n",
        "- Outputs results into a structured TSV file.\n",
        "\"\"\"\n",
        "\n",
        "import json\n",
        "import csv\n",
        "import random\n",
        "from datetime import datetime\n",
        "\n",
        "books_json_path = \"./ArabicBookGraph.json\"\n",
        "input_tsv = \"./recommendation_explanations - Copy.tsv\"\n",
        "output_tsv = \"./recommendation_results_short_only.tsv\"\n",
        "\n",
        "# قوالب الشرح المختصر فقط\n",
        "SHORT_EXPLANATION_TEMPLATES = {\n",
        "    \"author\": [\n",
        "        \"يناسبك '{title}' لأنك معجب بأسلوب {author} المميز\",\n",
        "        \"بما أنك من محبي أعمال {author}، فإن '{title}' سيثير اهتمامك\"\n",
        "    ],\n",
        "    \"genre\": [\n",
        "        \"'{title}' يقع ضمن تصنيفك المفضل من الكتب\",\n",
        "        \"بناءً على تفضيلاتك لهذا النوع الأدبي، نوصي بـ '{title}'\"\n",
        "    ],\n",
        "    \"similar_users\": [\n",
        "        \"مستخدمون يشبهونك في الذوق الأدبي أعجبهم '{title}'\",\n",
        "        \"حسب تفضيلات المستخدمين المشابهين لك، قد يعجبك '{title}'\"\n",
        "    ],\n",
        "    \"general\": [\n",
        "        \"قد تجد في '{title}' ما يثير اهتمامك\",\n",
        "        \"نوصيك بقراءة '{title}' لما يقدمه من محتوى قيم\"\n",
        "    ]\n",
        "}\n",
        "\n",
        "def load_books_data(json_path):\n",
        "    try:\n",
        "        with open(json_path, \"r\", encoding=\"utf-8\") as f:\n",
        "            return json.load(f)\n",
        "    except Exception as e:\n",
        "        print(f\"خطأ في تحميل ملف الكتب: {e}\")\n",
        "        return []\n",
        "\n",
        "def extract_book_info(book_id, books_data):\n",
        "    for book in books_data:\n",
        "        if book.get(\"id\") == str(book_id):\n",
        "            title = book.get(\"book_name\", \"غير معروف\")\n",
        "            author = book.get(\"author_name\", \"غير معروف\")\n",
        "            genres = book.get(\"genres\")\n",
        "            genre = genres[0] if isinstance(genres, list) and genres else book.get(\"genre\", \"غير معروف\")\n",
        "            return title, author, genre\n",
        "    return \"غير معروف\", \"غير معروف\", \"غير معروف\"\n",
        "\n",
        "def analyze_path(path):\n",
        "    p = path.replace('\"', '').strip().lower()\n",
        "    if \"written_by-author\" in p:\n",
        "        return \"author\"\n",
        "    if \"categorized_as-genre\" in p:\n",
        "        return \"genre\"\n",
        "    if \"similar_to-user\" in p:\n",
        "        return \"similar_users\"\n",
        "    return \"general\"\n",
        "\n",
        "def format_short_explanation(title, author, genre, rec_type):\n",
        "    templates = SHORT_EXPLANATION_TEMPLATES.get(rec_type, SHORT_EXPLANATION_TEMPLATES[\"general\"])\n",
        "    return random.choice(templates).format(title=title, author=author, genre=genre)\n",
        "\n",
        "def process_recommendations():\n",
        "    books_data = load_books_data(books_json_path)\n",
        "    if not books_data:\n",
        "        print(\"لا يمكن المتابعة بدون بيانات الكتب\")\n",
        "        return\n",
        "\n",
        "    with open(input_tsv, \"r\", encoding=\"utf-8-sig\", newline='') as infile, \\\n",
        "         open(output_tsv, \"w\", encoding=\"utf-8\", newline='') as outfile:\n",
        "\n",
        "        reader = csv.reader(infile, delimiter='\\t')\n",
        "        writer = csv.writer(outfile, delimiter='\\t')\n",
        "        outfile.write('\\ufeff')  # BOM للغة العربية\n",
        "        writer.writerow([\n",
        "            \"user_id\",\n",
        "            \"book_id\",\n",
        "            \"recommendation_type\",\n",
        "            \"short_explanation\",\n",
        "            \"generation_date\"\n",
        "        ])\n",
        "\n",
        "        next(reader, None)  # تجاوز العنوان\n",
        "        for idx, row in enumerate(reader, start=1):\n",
        "            if len(row) < 3:\n",
        "                continue\n",
        "\n",
        "            user_id, book_id, path = row[:3]\n",
        "            rec_type = analyze_path(path)\n",
        "            title, author, genre = extract_book_info(book_id, books_data)\n",
        "\n",
        "            short = format_short_explanation(title, author, genre, rec_type)\n",
        "\n",
        "            writer.writerow([\n",
        "                user_id,\n",
        "                book_id,\n",
        "                rec_type,\n",
        "                short,\n",
        "                datetime.now().strftime('%Y-%m-%d')\n",
        "            ])\n",
        "\n",
        "            if idx % 10 == 0:\n",
        "                print(f\"تمت معالجة {idx} توصيات...\")\n",
        "\n",
        "    print(f\"انتهى التنفيذ وحُفظت النتائج في: {output_tsv}\")\n"
      ],
      "metadata": {
        "id": "RjfwN9RKErY1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "LAOAsmidErcv"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}